{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm5sYZnt-dnV"
   },
   "source": [
    "# instsll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_ebO6foOD1u",
    "outputId": "c96ac937-5093-465b-c5e5-caea49fd9b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pmdarima in c:\\users\\pppat\\anaconda3\\lib\\site-packages (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (0.14.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (2.2.2)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (69.5.1)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pmdarima) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from pandas>=0.19->pmdarima) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->pmdarima) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
      "Requirement already satisfied: six in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHec8da_OEPt",
    "outputId": "e6a98d51-d53e-449f-896f-b7d8271fb9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pppat\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pppat\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bNVTrLcm97tq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import additional necessary libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXjgHbinOPwB"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7D3BvLfadph"
   },
   "source": [
    "# clean Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4x_hiSXlah4b",
    "outputId": "39c7c75c-ede9-4f49-9965-3c1447eab154"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Rain-N1-2003-1998.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Iterate over the file paths and concatenate the DataFrames\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m rainfall_files:\n\u001b[1;32m---> 18\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)  \u001b[38;5;66;03m# Read each CSV file\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     rainfall_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([rainfall_data, temp_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Concatenate to the main DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Display the first 10 rows of the concatenated DataFrame\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Rain-N1-2003-1998.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths (make sure these match the uploaded files)\n",
    "rainfall_files = [\n",
    "    '/content/Rain-N1-2003-1998.csv',\n",
    "    '/content/Rain-N1-2013-2004.csv',\n",
    "    '/content/Rain-N1-2023-2014.csv',\n",
    "    '/content/Rain-N2-2003-1998.csv',\n",
    "    '/content/Rain-N2-2013-2004.csv',\n",
    "    '/content/Rain-N2-2023-2014.csv'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the concatenated data\n",
    "rainfall_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file paths and concatenate the DataFrames\n",
    "for file_path in rainfall_files:\n",
    "    temp_df = pd.read_csv(file_path)  # Read each CSV file\n",
    "    rainfall_data = pd.concat([rainfall_data, temp_df], ignore_index=True)  # Concatenate to the main DataFrame\n",
    "\n",
    "# Display the first 10 rows of the concatenated DataFrame\n",
    "rainfall_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jfhxu5uahLo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "C0J4zqKUcKJ2",
    "outputId": "b02c4bc6-a2df-491a-bf35-685d9548ad11"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบค่าว่างใน DataFrame\n",
    "null_values = rainfall_data.isnull()\n",
    "\n",
    "# แสดงผลลัพธ์การตรวจสอบค่าที่หายไป\n",
    "null_values.head(10) # แสดง 10 แถวแรก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DqVBMsCcY98",
    "outputId": "715a6c81-82f2-4fd0-b05d-aae63a3b3d6b"
   },
   "outputs": [],
   "source": [
    "# นับจำนวนค่าที่หายไปในแต่ละคอลัมน์และแสดงผลลัพธ์\n",
    "print(rainfall_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gUlDQ9lckaE",
    "outputId": "a78916cd-b958-420a-8e13-dfe5cce9f193"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_rainfall_data = rainfall_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_rainfall_data.head(10)\n",
    "\n",
    "# แสดงขนาดของ DataFrame ใหม่\n",
    "cleaned_rainfall_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Q7YBbTZtdNoF",
    "outputId": "0ea97780-5007-45de-e72f-43178490bcfa"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_rainfall_data = rainfall_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_rainfall_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DGRIvQX9dWgn",
    "outputId": "ea23665b-cba3-4520-9310-53d32199fccd"
   },
   "outputs": [],
   "source": [
    "cleaned_rainfall_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ukg9Pd7dmCZ",
    "outputId": "320db690-777e-403e-b657-84e5aeeb6c37"
   },
   "outputs": [],
   "source": [
    "cleaned_rainfall_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c6HYdYrfd_lB",
    "outputId": "bef74839-27f3-4443-9439-c63fc7df7a01"
   },
   "outputs": [],
   "source": [
    "cleaned_rainfall_data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJxxbgQkeX_E",
    "outputId": "ed7b6ff6-25af-4677-a1c5-725cdeadb392"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบจำนวนค่าที่หายไปในแต่ละคอลัมน์ของ DataFrame ที่ทำความสะอาดแล้ว\n",
    "missing_values_summary = cleaned_rainfall_data.isnull().sum()\n",
    "\n",
    "# แสดงผลลัพธ์ของจำนวนค่าที่หายไปในแต่ละคอลัมน์\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hnYm5bRbhr19",
    "outputId": "3f1f86be-a620-4a6e-bc84-9eb307365f18"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์ใน DataFrame\n",
    "cleaned_rainfall_data.rename(\n",
    "    columns={\n",
    "        \"ปริมาณฝน(มิลลิเมตร)\": \"NO\",\n",
    "        \"Unnamed: 1\": \"STATION\",\n",
    "        \"Unnamed: 2\": \"Datetime\",\n",
    "        \"Unnamed: 3\": \"Day1\",\n",
    "        \"Unnamed: 4\": \"Day2\",\n",
    "        \"Unnamed: 5\": \"Day3\",\n",
    "        \"Unnamed: 6\": \"Day4\",\n",
    "        \"Unnamed: 7\": \"Day5\",\n",
    "        \"Unnamed: 8\": \"Day6\",\n",
    "        \"Unnamed: 9\": \"Day7\",\n",
    "        \"Unnamed: 10\": \"Day8\",\n",
    "        \"Unnamed: 11\": \"Day9\",\n",
    "        \"Unnamed: 12\": \"Day10\",\n",
    "        \"Unnamed: 13\": \"Day11\",\n",
    "        \"Unnamed: 14\": \"Day12\",\n",
    "        \"Unnamed: 15\": \"Day13\",\n",
    "        \"Unnamed: 16\": \"Day14\",\n",
    "        \"Unnamed: 17\": \"Day15\",\n",
    "        \"Unnamed: 18\": \"Day16\",\n",
    "        \"Unnamed: 19\": \"Day17\",\n",
    "        \"Unnamed: 20\": \"Day18\",\n",
    "        \"Unnamed: 21\": \"Day19\",\n",
    "        \"Unnamed: 22\": \"Day20\",\n",
    "        \"Unnamed: 23\": \"Day21\",\n",
    "        \"Unnamed: 24\": \"Day22\",\n",
    "        \"Unnamed: 25\": \"Day23\",\n",
    "        \"Unnamed: 26\": \"Day24\",\n",
    "        \"Unnamed: 27\": \"Day25\",\n",
    "        \"Unnamed: 28\": \"Day26\",\n",
    "        \"Unnamed: 29\": \"Day27\",\n",
    "        \"Unnamed: 30\": \"Day28\",\n",
    "        \"Unnamed: 31\": \"Day29\",\n",
    "        \"Unnamed: 32\": \"Day30\",\n",
    "        \"Unnamed: 33\": \"Day31\",\n",
    "        \"Unnamed: 34\": \"Rainfall\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# แสดง DataFrame หลังจากเปลี่ยนชื่อคอลัมน์\n",
    "cleaned_rainfall_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7rAMX0Y_SNd",
    "outputId": "77259d14-80f9-4fe5-98d6-6fb30880695a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_rainfall_data['Datetime'] = pd.to_datetime(cleaned_rainfall_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_rainfall_data['Year'] = cleaned_rainfall_data['Datetime'].dt.year\n",
    "cleaned_rainfall_data['Month'] = cleaned_rainfall_data['Datetime'].dt.month\n",
    "\n",
    "# ตรวจสอบปีที่หายไปในแต่ละสถานี\n",
    "missing_years_list = []\n",
    "for station in cleaned_rainfall_data['STATION'].unique():\n",
    "    # สร้างชุดปีทั้งหมดที่มีในข้อมูล\n",
    "    all_years = set(cleaned_rainfall_data['Year'])\n",
    "    years_with_data = set(cleaned_rainfall_data[cleaned_rainfall_data['STATION'] == station]['Year'])\n",
    "    missing_years_in_station = all_years - years_with_data\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในแต่ละปีที่มีข้อมูล\n",
    "    for year in missing_years_in_station:\n",
    "        missing_years_list.append({\n",
    "            'STATION': station,\n",
    "            'Year': year,\n",
    "            'Missing Months': 'All months missing'\n",
    "        })\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = cleaned_rainfall_data[(cleaned_rainfall_data['STATION'] == station) & (cleaned_rainfall_data['Year'] == year)]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        if missing_months:\n",
    "            missing_years_list.append({\n",
    "                'STATION': station,\n",
    "                'Year': year,\n",
    "                'Missing Months': sorted(missing_months)\n",
    "            })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_years_df = pd.DataFrame(missing_years_list)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_years_df.empty:\n",
    "    print(\"มีสถานีที่มีข้อมูลปีและเดือนหายไป:\")\n",
    "    print(missing_years_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลปีและเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KsenWAgif5h",
    "outputId": "eddf870b-e374-4f15-cac5-b68adcbca302"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_rainfall_data['Datetime'] = pd.to_datetime(cleaned_rainfall_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_rainfall_data['Year'] = cleaned_rainfall_data['Datetime'].dt.year\n",
    "cleaned_rainfall_data['Month'] = cleaned_rainfall_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "missing_months_summary = []\n",
    "\n",
    "for station in cleaned_rainfall_data['STATION'].unique():\n",
    "    station_data = cleaned_rainfall_data[cleaned_rainfall_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "    missing_percentage = (missing_months_count / total_months) * 100\n",
    "\n",
    "    # เก็บข้อมูลผลลัพธ์\n",
    "    missing_months_summary.append({\n",
    "        'STATION': station,\n",
    "        'Total Missing Months': missing_months_count,\n",
    "        'Percentage Missing': missing_percentage\n",
    "    })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_months_df = pd.DataFrame(missing_months_summary)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_months_df.empty:\n",
    "    print(\"ข้อมูลเปอร์เซ็นต์ของเดือนที่หายไปสำหรับแต่ละสถานี:\")\n",
    "    print(missing_months_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hweAfPem5-54",
    "outputId": "d6a564ae-6c90-4d62-f9b4-107540043165"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_rainfall_data['Datetime'] = pd.to_datetime(cleaned_rainfall_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_rainfall_data['Year'] = cleaned_rainfall_data['Datetime'].dt.year\n",
    "cleaned_rainfall_data['Month'] = cleaned_rainfall_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "total_months_all_stations = 0\n",
    "missing_months_all_stations = 0\n",
    "\n",
    "for station in cleaned_rainfall_data['STATION'].unique():\n",
    "    station_data = cleaned_rainfall_data[cleaned_rainfall_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # อัปเดตค่าทั้งหมดสำหรับทุกสถานี\n",
    "    total_months_all_stations += total_months\n",
    "    missing_months_all_stations += missing_months_count\n",
    "\n",
    "# คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "missing_percentage_all_stations = (missing_months_all_stations / total_months_all_stations) * 100\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(f\"รวมข้อมูลสำหรับทุกสถานีในช่วงปี {start_year}-{end_year}:\")\n",
    "print(f\"จำนวนเดือนทั้งหมดที่คาดหวัง: {total_months_all_stations}\")\n",
    "print(f\"จำนวนเดือนที่หายไป: {missing_months_all_stations}\")\n",
    "print(f\"เปอร์เซ็นต์ของเดือนที่หายไป: {missing_percentage_all_stations:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DMBwkfs6mEOW",
    "outputId": "2646a16e-5498-44c1-c624-b7b9a3c03852"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนคอลัมน์ 'Datetime' ใน cleaned_rainfall_data เป็นประเภทวันที่\n",
    "cleaned_rainfall_data['Datetime'] = pd.to_datetime(cleaned_rainfall_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_rainfall_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Cu2C9leOnE91",
    "outputId": "61e099cb-e5a0-43b1-8ecb-38b261510034"
   },
   "outputs": [],
   "source": [
    "# สร้างคอลัมน์ 'YEAR' จากคอลัมน์ 'Datetime'\n",
    "cleaned_rainfall_data['YEAR'] = cleaned_rainfall_data['Datetime'].dt.year\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_rainfall_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "62BSx9QLnVaC",
    "outputId": "a335499f-0935-4332-ef4a-2258c133d6b8"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์เป็นตัวพิมพ์ใหญ่\n",
    "cleaned_rainfall_data.columns = [col.upper() for col in cleaned_rainfall_data.columns]\n",
    "\n",
    "# กำหนดชื่อคอลัมน์ใหม่ (ตัวพิมพ์ใหญ่)\n",
    "new_columns = [\n",
    "    'NO', 'STATION', 'DATETIME',\n",
    "    'DAY1', 'DAY2', 'DAY3', 'DAY4', 'DAY5', 'DAY6', 'DAY7', 'DAY8', 'DAY9',\n",
    "    'DAY10', 'DAY11', 'DAY12', 'DAY13', 'DAY14', 'DAY15', 'DAY16', 'DAY17',\n",
    "    'DAY18', 'DAY19', 'DAY20', 'DAY21', 'DAY22', 'DAY23', 'DAY24', 'DAY25',\n",
    "    'DAY26', 'DAY27', 'DAY28', 'DAY29', 'DAY30', 'DAY31', 'RAINFALL'\n",
    "]\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่ต้องการใน DataFrame\n",
    "cleaned_rainfall_data = cleaned_rainfall_data[new_columns]\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_rainfall_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IqizxJNndj7",
    "outputId": "4693c565-3e28-48f7-c699-30a75771d0a8"
   },
   "outputs": [],
   "source": [
    "cleaned_rainfall_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1OCUpMJ5qA8B",
    "outputId": "6b1e3058-73e7-4da8-8371-af633cdccdcc"
   },
   "outputs": [],
   "source": [
    "cleaned_rainfall_data['NO'] = range(1, len(cleaned_rainfall_data) + 1)\n",
    "cleaned_rainfall_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoZB1igC2_m2",
    "outputId": "3867af9a-9cb5-45fc-babb-315610c36479"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = [f'DAY{i}' for i in range(1, 32)] + ['NO']\n",
    "\n",
    "# ลบคอลัมน์ที่ระบุจาก DataFrame cleaned_rainfall_data\n",
    "cleaned_rainfall_data = cleaned_rainfall_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# รีเซ็ตดัชนีของ DataFrame\n",
    "cleaned_rainfall_data = cleaned_rainfall_data.reset_index(drop=True)\n",
    "\n",
    "# ตรวจสอบผลลัพธ์\n",
    "print(cleaned_rainfall_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COaXIv6FS5rQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# clean Evaporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tqCJt9CyS5rQ",
    "outputId": "77c1d8ae-d353-4266-f88a-f8b14211fd98"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths (make sure these match the uploaded files)\n",
    "file_paths = [\n",
    "    '/content/eva-d-N-1-1998-2003.csv',\n",
    "    '/content/eva-d-N-1-2004-2013.csv',\n",
    "    '/content/eva-d-N-1-2014-2023.csv',\n",
    "    '/content/eva-d-N-2-1998-2003.csv',\n",
    "    '/content/eva-d-N-2-2004-2013.csv',\n",
    "    '/content/eva-d-N-2-2014-2023.csv'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the concatenated data\n",
    "evaporation_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file paths and concatenate the DataFrames\n",
    "for file_path in file_paths:\n",
    "    temp_df = pd.read_csv(file_path)  # Read each CSV file\n",
    "    evaporation_data = pd.concat([evaporation_data, temp_df], ignore_index=True)  # Concatenate to the main DataFrame\n",
    "\n",
    "# Display the first 10 rows of the concatenated DataFrame\n",
    "evaporation_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uktG83yyS5rQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RubGCAhwS5rQ",
    "outputId": "2a32d23d-6a19-4de2-cd74-9230607e9c49"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบค่าว่างใน DataFrame\n",
    "null_values = evaporation_data.isnull()\n",
    "\n",
    "# แสดงผลลัพธ์การตรวจสอบค่าที่หายไป\n",
    "null_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aid9UXKOS5rQ",
    "outputId": "3108fd74-f595-4ff8-cbdb-eec5c51306fb"
   },
   "outputs": [],
   "source": [
    "# นับจำนวนค่าที่หายไปในแต่ละคอลัมน์และแสดงผลลัพธ์\n",
    "print(evaporation_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KQ7KOuQS5rR",
    "outputId": "10c2ec40-bda6-436b-ef19-4cdb5f4c7cdd"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_evaporation_data = evaporation_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_evaporation_data.head(10)\n",
    "\n",
    "# แสดงขนาดของ DataFrame ใหม่\n",
    "cleaned_evaporation_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2EOd39dbS5rR",
    "outputId": "e06249e4-51b8-44e5-cae1-36065b6ab3ea"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_evaporation_data = evaporation_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_evaporation_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "W_JEjw6zS5rR",
    "outputId": "6ea533e3-848d-4bd4-ac5a-0c44d6b3381a"
   },
   "outputs": [],
   "source": [
    "cleaned_evaporation_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3U0s8pXS5rR",
    "outputId": "b568d23a-ba78-4f31-e95e-2f38d284e2c5"
   },
   "outputs": [],
   "source": [
    "cleaned_evaporation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7R5d0lXfS5rR",
    "outputId": "9b569fb7-fb54-4b0f-a252-0e543c9a1669"
   },
   "outputs": [],
   "source": [
    "cleaned_evaporation_data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8n1SDQmS5rR",
    "outputId": "35f6739e-8fc9-4966-8f17-40ce34d04c66"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบจำนวนค่าที่หายไปในแต่ละคอลัมน์ของ DataFrame ที่ทำความสะอาดแล้ว\n",
    "missing_values_summary = cleaned_evaporation_data.isnull().sum()\n",
    "\n",
    "# แสดงผลลัพธ์ของจำนวนค่าที่หายไปในแต่ละคอลัมน์\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PIAbF_CjS5rS",
    "outputId": "bc6d8c8e-2788-475e-a99c-136c2a2b1a9e"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์ใน DataFrame\n",
    "cleaned_evaporation_data.rename(\n",
    "    columns={\n",
    "        \"ปริมาณฝน(มิลลิเมตร)\": \"NO\",\n",
    "        \"Unnamed: 1\": \"STATION\",\n",
    "        \"Unnamed: 2\": \"Datetime\",\n",
    "        \"Unnamed: 3\": \"Day1\",\n",
    "        \"Unnamed: 4\": \"Day2\",\n",
    "        \"Unnamed: 5\": \"Day3\",\n",
    "        \"Unnamed: 6\": \"Day4\",\n",
    "        \"Unnamed: 7\": \"Day5\",\n",
    "        \"Unnamed: 8\": \"Day6\",\n",
    "        \"Unnamed: 9\": \"Day7\",\n",
    "        \"Unnamed: 10\": \"Day8\",\n",
    "        \"Unnamed: 11\": \"Day9\",\n",
    "        \"Unnamed: 12\": \"Day10\",\n",
    "        \"Unnamed: 13\": \"Day11\",\n",
    "        \"Unnamed: 14\": \"Day12\",\n",
    "        \"Unnamed: 15\": \"Day13\",\n",
    "        \"Unnamed: 16\": \"Day14\",\n",
    "        \"Unnamed: 17\": \"Day15\",\n",
    "        \"Unnamed: 18\": \"Day16\",\n",
    "        \"Unnamed: 19\": \"Day17\",\n",
    "        \"Unnamed: 20\": \"Day18\",\n",
    "        \"Unnamed: 21\": \"Day19\",\n",
    "        \"Unnamed: 22\": \"Day20\",\n",
    "        \"Unnamed: 23\": \"Day21\",\n",
    "        \"Unnamed: 24\": \"Day22\",\n",
    "        \"Unnamed: 25\": \"Day23\",\n",
    "        \"Unnamed: 26\": \"Day24\",\n",
    "        \"Unnamed: 27\": \"Day25\",\n",
    "        \"Unnamed: 28\": \"Day26\",\n",
    "        \"Unnamed: 29\": \"Day27\",\n",
    "        \"Unnamed: 30\": \"Day28\",\n",
    "        \"Unnamed: 31\": \"Day29\",\n",
    "        \"Unnamed: 32\": \"Day30\",\n",
    "        \"Unnamed: 33\": \"Day31\",\n",
    "        \"Unnamed: 34\": \"Evaporation\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# แสดง DataFrame หลังจากเปลี่ยนชื่อคอลัมน์\n",
    "cleaned_evaporation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLWu0qjxS5rS",
    "outputId": "d8233488-44ba-4d5a-acb2-f4783b4963df"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_evaporation_data['Datetime'] = pd.to_datetime(cleaned_evaporation_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_evaporation_data['Year'] = cleaned_evaporation_data['Datetime'].dt.year\n",
    "cleaned_evaporation_data['Month'] = cleaned_evaporation_data['Datetime'].dt.month\n",
    "\n",
    "# ตรวจสอบปีที่หายไปในแต่ละสถานี\n",
    "missing_years_list = []\n",
    "for station in cleaned_evaporation_data['STATION'].unique():\n",
    "    # สร้างชุดปีทั้งหมดที่มีในข้อมูล\n",
    "    all_years = set(cleaned_evaporation_data['Year'])\n",
    "    years_with_data = set(cleaned_evaporation_data[cleaned_evaporation_data['STATION'] == station]['Year'])\n",
    "    missing_years_in_station = all_years - years_with_data\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในแต่ละปีที่มีข้อมูล\n",
    "    for year in missing_years_in_station:\n",
    "        missing_years_list.append({\n",
    "            'STATION': station,\n",
    "            'Year': year,\n",
    "            'Missing Months': 'All months missing'\n",
    "        })\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = cleaned_evaporation_data[(cleaned_evaporation_data['STATION'] == station) & (cleaned_evaporation_data['Year'] == year)]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        if missing_months:\n",
    "            missing_years_list.append({\n",
    "                'STATION': station,\n",
    "                'Year': year,\n",
    "                'Missing Months': sorted(missing_months)\n",
    "            })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_years_df = pd.DataFrame(missing_years_list)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_years_df.empty:\n",
    "    print(\"มีสถานีที่มีข้อมูลปีและเดือนหายไป:\")\n",
    "    print(missing_years_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลปีและเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnuqrbP8_h2-",
    "outputId": "acfc4faf-eba4-4aa3-cae0-4065f57cda20"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_evaporation_data['Datetime'] = pd.to_datetime(cleaned_evaporation_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_evaporation_data['Year'] = cleaned_evaporation_data['Datetime'].dt.year\n",
    "cleaned_evaporation_data['Month'] = cleaned_evaporation_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "missing_months_summary = []\n",
    "\n",
    "for station in cleaned_evaporation_data['STATION'].unique():\n",
    "    station_data = cleaned_evaporation_data[cleaned_evaporation_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "    missing_percentage = (missing_months_count / total_months) * 100\n",
    "\n",
    "    # เก็บข้อมูลผลลัพธ์\n",
    "    missing_months_summary.append({\n",
    "        'STATION': station,\n",
    "        'Total Missing Months': missing_months_count,\n",
    "        'Percentage Missing': missing_percentage\n",
    "    })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_months_df = pd.DataFrame(missing_months_summary)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_months_df.empty:\n",
    "    print(\"ข้อมูลเปอร์เซ็นต์ของเดือนที่หายไปสำหรับแต่ละสถานี:\")\n",
    "    print(missing_months_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MiwYhyPtAHKw",
    "outputId": "931e235a-cdbe-498a-a9a6-7fa81675dcd8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_evaporation_data['Datetime'] = pd.to_datetime(cleaned_evaporation_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_evaporation_data['Year'] = cleaned_evaporation_data['Datetime'].dt.year\n",
    "cleaned_evaporation_data['Month'] = cleaned_evaporation_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "total_months_all_stations = 0\n",
    "missing_months_all_stations = 0\n",
    "\n",
    "for station in cleaned_evaporation_data['STATION'].unique():\n",
    "    station_data = cleaned_evaporation_data[cleaned_evaporation_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # อัปเดตค่าทั้งหมดสำหรับทุกสถานี\n",
    "    total_months_all_stations += total_months\n",
    "    missing_months_all_stations += missing_months_count\n",
    "\n",
    "# คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "missing_percentage_all_stations = (missing_months_all_stations / total_months_all_stations) * 100\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(f\"รวมข้อมูลสำหรับทุกสถานีในช่วงปี {start_year}-{end_year}:\")\n",
    "print(f\"จำนวนเดือนทั้งหมดที่คาดหวัง: {total_months_all_stations}\")\n",
    "print(f\"จำนวนเดือนที่หายไป: {missing_months_all_stations}\")\n",
    "print(f\"เปอร์เซ็นต์ของเดือนที่หายไป: {missing_percentage_all_stations:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DU_d01y4S5rS",
    "outputId": "470d5eca-6ef7-4934-e661-d4f15816ac48"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนคอลัมน์ 'Datetime' ใน cleaned_rainfall_data เป็นประเภทวันที่\n",
    "cleaned_evaporation_data['Datetime'] = pd.to_datetime(cleaned_evaporation_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_evaporation_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Ve119YXDS5rS",
    "outputId": "ef03e562-44b3-4e29-dbc5-0ca8c3ed2900"
   },
   "outputs": [],
   "source": [
    "# สร้างคอลัมน์ 'YEAR' จากคอลัมน์ 'Datetime'\n",
    "cleaned_evaporation_data['YEAR'] = cleaned_evaporation_data['Datetime'].dt.year\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_evaporation_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nt0bXX99S5rS",
    "outputId": "91447322-09af-435d-afff-c4b32d7c5f44"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์เป็นตัวพิมพ์ใหญ่\n",
    "cleaned_evaporation_data.columns = [col.upper() for col in cleaned_evaporation_data.columns]\n",
    "\n",
    "# กำหนดชื่อคอลัมน์ใหม่ (ตัวพิมพ์ใหญ่)\n",
    "new_columns = [\n",
    "    'STATION', 'DATETIME',\n",
    "    'DAY1', 'DAY2', 'DAY3', 'DAY4', 'DAY5', 'DAY6', 'DAY7', 'DAY8', 'DAY9',\n",
    "    'DAY10', 'DAY11', 'DAY12', 'DAY13', 'DAY14', 'DAY15', 'DAY16', 'DAY17',\n",
    "    'DAY18', 'DAY19', 'DAY20', 'DAY21', 'DAY22', 'DAY23', 'DAY24', 'DAY25',\n",
    "    'DAY26', 'DAY27', 'DAY28', 'DAY29', 'DAY30', 'DAY31', 'EVAPORATION'\n",
    "]\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่ต้องการใน DataFrame\n",
    "cleaned_evaporation_data = cleaned_evaporation_data[new_columns]\n",
    "\n",
    "# สร้างคอลัมน์ 'NO' และเพิ่ม \"NO\" ในทุกแถว\n",
    "cleaned_evaporation_data['NO'] = 'NO'\n",
    "\n",
    "# จัดเรียงคอลัมน์ใหม่ให้อยู่ในลำดับที่ต้องการ\n",
    "cleaned_evaporation_data = cleaned_evaporation_data[['NO'] + new_columns]\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_evaporation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5QVzbdhS5rT",
    "outputId": "372301a1-80a3-4673-f055-98661c56b9ea"
   },
   "outputs": [],
   "source": [
    "cleaned_evaporation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jpn2InQSS5rT",
    "outputId": "e911f804-cc01-4514-8494-256399bc5632"
   },
   "outputs": [],
   "source": [
    "cleaned_evaporation_data['NO'] = range(1, len(cleaned_evaporation_data) + 1)\n",
    "cleaned_evaporation_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS8j4ejV9WBa",
    "outputId": "2e4a94f4-4f95-4f79-a407-00721e4162e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = [f'DAY{i}' for i in range(1, 32)] + ['NO']\n",
    "\n",
    "# ลบคอลัมน์ที่ระบุจาก DataFrame cleaned_rainfall_data\n",
    "cleaned_evaporation_data = cleaned_evaporation_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# รีเซ็ตดัชนีของ DataFrame\n",
    "cleaned_evaporation_data = cleaned_evaporation_data.reset_index(drop=True)\n",
    "\n",
    "# ตรวจสอบผลลัพธ์\n",
    "print(cleaned_evaporation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK_lDvvFS6SY"
   },
   "source": [
    "# clean Rain Tem low hight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ksuf4VDZjot",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TEMhigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7Wv-y5PUS6SY",
    "outputId": "c9f6d54b-a6e3-4231-e835-1ff026c725f7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths (make sure these match the uploaded files)\n",
    "temperature_files = [\n",
    "    '/content/MaxT-N1-2003-1998.csv',\n",
    "    '/content/MaxT-N1-2013-2004.csv',\n",
    "    '/content/MaxT-N1-2023-2014.csv',\n",
    "    '/content/MaxT-N2-2003-1998.csv',\n",
    "    '/content/MaxT-N2-2013-2004.csv',\n",
    "    '/content/MaxT-N2-2023-2014.csv'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the concatenated data\n",
    "temperature_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file paths and concatenate the DataFrames\n",
    "for file_path in temperature_files:\n",
    "    temp_df = pd.read_csv(file_path)  # Read each CSV file\n",
    "    temperature_data = pd.concat([temperature_data, temp_df], ignore_index=True)  # Concatenate to the main DataFrame\n",
    "\n",
    "# Display the first 10 rows of the concatenated DataFrame\n",
    "temperature_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "a6k7-ZvQS6SZ",
    "outputId": "5c3d0c8f-f03c-4c7e-bbc9-952cfb634e82"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบค่าว่างใน DataFrame\n",
    "null_values = temperature_data.isnull()\n",
    "\n",
    "# แสดงผลลัพธ์การตรวจสอบค่าที่หายไป\n",
    "null_values.head(10)  # แสดง 10 แถวแรก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gk0OmsY5S6SZ",
    "outputId": "e415458b-63a3-4a55-a5d4-9bff0800396d"
   },
   "outputs": [],
   "source": [
    "# นับจำนวนค่าที่หายไปในแต่ละคอลัมน์และแสดงผลลัพธ์\n",
    "print(temperature_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEilrU08S6Sa",
    "outputId": "fb5fc332-bd05-4b13-e435-585f7fe9fd06"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_temperature_data = temperature_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_temperature_data.head(10)\n",
    "\n",
    "# แสดงขนาดของ DataFrame ใหม่\n",
    "cleaned_temperature_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eCWhvHGCS6Sa",
    "outputId": "9faca1b5-1468-4b1f-f9ad-3c3ef62c95bc"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_temperature_data = temperature_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_temperature_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "L8lxiJD_S6Sb",
    "outputId": "bdc9fe49-ae8d-4fe0-861d-72b572223860"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Izzag4gvS6Sb",
    "outputId": "095060eb-8b06-4b97-d356-b874e826688a"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "TgkCpcrHS6Sb",
    "outputId": "a52f10ed-ffad-4f7e-9921-2976ce9e6463"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RWvj9IIS6Sb",
    "outputId": "f57dbcdb-ad9d-418b-c9f9-444d06ba1c82"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบจำนวนค่าที่หายไปในแต่ละคอลัมน์ของ DataFrame ที่ทำความสะอาดแล้ว\n",
    "missing_values_summary = cleaned_temperature_data.isnull().sum()\n",
    "\n",
    "# แสดงผลลัพธ์ของจำนวนค่าที่หายไปในแต่ละคอลัมน์\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "quvvRRa7S6Sc",
    "outputId": "a15e177b-8814-41d2-c6ad-0038eba4cb50"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์ใน DataFrame\n",
    "cleaned_temperature_data.rename(\n",
    "    columns={\n",
    "        \"ปริมาณฝน(มิลลิเมตร)\": \"NO\",\n",
    "        \"Unnamed: 1\": \"STATION\",\n",
    "        \"Unnamed: 2\": \"Datetime\",\n",
    "        \"Unnamed: 3\": \"Day1\",\n",
    "        \"Unnamed: 4\": \"Day2\",\n",
    "        \"Unnamed: 5\": \"Day3\",\n",
    "        \"Unnamed: 6\": \"Day4\",\n",
    "        \"Unnamed: 7\": \"Day5\",\n",
    "        \"Unnamed: 8\": \"Day6\",\n",
    "        \"Unnamed: 9\": \"Day7\",\n",
    "        \"Unnamed: 10\": \"Day8\",\n",
    "        \"Unnamed: 11\": \"Day9\",\n",
    "        \"Unnamed: 12\": \"Day10\",\n",
    "        \"Unnamed: 13\": \"Day11\",\n",
    "        \"Unnamed: 14\": \"Day12\",\n",
    "        \"Unnamed: 15\": \"Day13\",\n",
    "        \"Unnamed: 16\": \"Day14\",\n",
    "        \"Unnamed: 17\": \"Day15\",\n",
    "        \"Unnamed: 18\": \"Day16\",\n",
    "        \"Unnamed: 19\": \"Day17\",\n",
    "        \"Unnamed: 20\": \"Day18\",\n",
    "        \"Unnamed: 21\": \"Day19\",\n",
    "        \"Unnamed: 22\": \"Day20\",\n",
    "        \"Unnamed: 23\": \"Day21\",\n",
    "        \"Unnamed: 24\": \"Day22\",\n",
    "        \"Unnamed: 25\": \"Day23\",\n",
    "        \"Unnamed: 26\": \"Day24\",\n",
    "        \"Unnamed: 27\": \"Day25\",\n",
    "        \"Unnamed: 28\": \"Day26\",\n",
    "        \"Unnamed: 29\": \"Day27\",\n",
    "        \"Unnamed: 30\": \"Day28\",\n",
    "        \"Unnamed: 31\": \"Day29\",\n",
    "        \"Unnamed: 32\": \"Day30\",\n",
    "        \"Unnamed: 33\": \"Day31\",\n",
    "        \"Unnamed: 34\": \"average\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# แสดง DataFrame หลังจากเปลี่ยนชื่อคอลัมน์\n",
    "cleaned_temperature_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K807eT9AS6Sc",
    "outputId": "0a7489d3-b810-4544-b350-9d55befa38ab"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_data['Datetime'] = pd.to_datetime(cleaned_temperature_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_data['Year'] = cleaned_temperature_data['Datetime'].dt.year\n",
    "cleaned_temperature_data['Month'] = cleaned_temperature_data['Datetime'].dt.month\n",
    "\n",
    "# ตรวจสอบปีที่หายไปในแต่ละสถานี\n",
    "missing_years_list = []\n",
    "for station in cleaned_temperature_data['STATION'].unique():\n",
    "    # สร้างชุดปีทั้งหมดที่มีในข้อมูล\n",
    "    all_years = set(cleaned_temperature_data['Year'])\n",
    "    years_with_data = set(cleaned_temperature_data[cleaned_temperature_data['STATION'] == station]['Year'])\n",
    "    missing_years_in_station = all_years - years_with_data\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในแต่ละปีที่มีข้อมูล\n",
    "    for year in missing_years_in_station:\n",
    "        missing_years_list.append({\n",
    "            'STATION': station,\n",
    "            'Year': year,\n",
    "            'Missing Months': 'All months missing'\n",
    "        })\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = cleaned_temperature_data[(cleaned_temperature_data['STATION'] == station) & (cleaned_temperature_data['Year'] == year)]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        if missing_months:\n",
    "            missing_years_list.append({\n",
    "                'STATION': station,\n",
    "                'Year': year,\n",
    "                'Missing Months': sorted(missing_months)\n",
    "            })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_years_df = pd.DataFrame(missing_years_list)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_years_df.empty:\n",
    "    print(\"มีสถานีที่มีข้อมูลปีและเดือนหายไป:\")\n",
    "    print(missing_years_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลปีและเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9iPl46y_kDg",
    "outputId": "a8391d40-7da9-4b51-d548-022ce6549ecc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_data['Datetime'] = pd.to_datetime(cleaned_temperature_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_data['Year'] = cleaned_temperature_data['Datetime'].dt.year\n",
    "cleaned_temperature_data['Month'] = cleaned_temperature_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "missing_months_summary = []\n",
    "\n",
    "for station in cleaned_temperature_data['STATION'].unique():\n",
    "    station_data = cleaned_temperature_data[cleaned_temperature_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "    missing_percentage = (missing_months_count / total_months) * 100\n",
    "\n",
    "    # เก็บข้อมูลผลลัพธ์\n",
    "    missing_months_summary.append({\n",
    "        'STATION': station,\n",
    "        'Total Missing Months': missing_months_count,\n",
    "        'Percentage Missing': missing_percentage\n",
    "    })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_months_df = pd.DataFrame(missing_months_summary)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_months_df.empty:\n",
    "    print(\"ข้อมูลเปอร์เซ็นต์ของเดือนที่หายไปสำหรับแต่ละสถานี:\")\n",
    "    print(missing_months_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mo7pokcAJw1",
    "outputId": "06461e9e-f924-4886-a0fa-a63e95e61c65"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_data['Datetime'] = pd.to_datetime(cleaned_temperature_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_data['Year'] = cleaned_temperature_data['Datetime'].dt.year\n",
    "cleaned_temperature_data['Month'] = cleaned_temperature_data['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "total_months_all_stations = 0\n",
    "missing_months_all_stations = 0\n",
    "\n",
    "for station in cleaned_temperature_data['STATION'].unique():\n",
    "    station_data = cleaned_temperature_data[cleaned_temperature_data['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # อัปเดตค่าทั้งหมดสำหรับทุกสถานี\n",
    "    total_months_all_stations += total_months\n",
    "    missing_months_all_stations += missing_months_count\n",
    "\n",
    "# คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "missing_percentage_all_stations = (missing_months_all_stations / total_months_all_stations) * 100\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(f\"รวมข้อมูลสำหรับทุกสถานีในช่วงปี {start_year}-{end_year}:\")\n",
    "print(f\"จำนวนเดือนทั้งหมดที่คาดหวัง: {total_months_all_stations}\")\n",
    "print(f\"จำนวนเดือนที่หายไป: {missing_months_all_stations}\")\n",
    "print(f\"เปอร์เซ็นต์ของเดือนที่หายไป: {missing_percentage_all_stations:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "avFqAwiBS6Sc",
    "outputId": "5c81f243-aa62-46e9-fa5b-ded4662ada63"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนคอลัมน์ 'Datetime' ใน cleaned_rainfall_data เป็นประเภทวันที่\n",
    "cleaned_temperature_data['Datetime'] = pd.to_datetime(cleaned_temperature_data['Datetime'], format='%b-%y')\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "94IOimMGS6Sc",
    "outputId": "8cb31740-1d12-40d4-ff3f-f81edc29e3e4"
   },
   "outputs": [],
   "source": [
    "# สร้างคอลัมน์ 'YEAR' จากคอลัมน์ 'Datetime'\n",
    "cleaned_temperature_data['YEAR'] = cleaned_temperature_data['Datetime'].dt.year\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "oRiMLhFiS6Sd",
    "outputId": "dfe42190-90b9-4012-acc3-c0405a13c60b"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์เป็นตัวพิมพ์ใหญ่\n",
    "cleaned_temperature_data.columns = [col.upper() for col in cleaned_temperature_data.columns]\n",
    "\n",
    "# กำหนดชื่อคอลัมน์ใหม่ (ตัวพิมพ์ใหญ่)\n",
    "new_columns = [\n",
    "    'STATION', 'DATETIME',\n",
    "    'DAY1', 'DAY2', 'DAY3', 'DAY4', 'DAY5', 'DAY6', 'DAY7', 'DAY8', 'DAY9',\n",
    "    'DAY10', 'DAY11', 'DAY12', 'DAY13', 'DAY14', 'DAY15', 'DAY16', 'DAY17',\n",
    "    'DAY18', 'DAY19', 'DAY20', 'DAY21', 'DAY22', 'DAY23', 'DAY24', 'DAY25',\n",
    "    'DAY26', 'DAY27', 'DAY28', 'DAY29', 'DAY30', 'DAY31', 'AVERAGE'\n",
    "]\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่ต้องการใน DataFrame\n",
    "cleaned_temperature_data = cleaned_temperature_data[new_columns]\n",
    "\n",
    "# สร้างคอลัมน์ 'NO' และเพิ่ม \"NO\" ในทุกแถว\n",
    "cleaned_temperature_data['NO'] = 'NO'\n",
    "\n",
    "# จัดเรียงคอลัมน์ใหม่ให้อยู่ในลำดับที่ต้องการ\n",
    "cleaned_temperature_data = cleaned_temperature_data[['NO'] + new_columns]\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BW5ScVPWS6Sd",
    "outputId": "4081c1c4-81cc-4b68-9e97-10078cb21ac8"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "NxYSSXVbS6Sd",
    "outputId": "4fa1518a-4979-4890-9735-cda03b2f9461"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data['NO'] = range(1, len(cleaned_temperature_data) + 1)\n",
    "cleaned_temperature_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jC0ycYm0Yvby",
    "outputId": "ff92f676-08a9-4475-dfca-da98b1134fbd"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.drop(columns=['NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJieuvFsdsX5",
    "outputId": "819d5a29-acba-4f7c-f3da-d68f65124f6f"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5w2LNkMgkqx",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TEMlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1i5_z8_Rgkqy",
    "outputId": "e3da15a9-6116-4a9c-bc13-335b67577e0c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths (make sure these match the uploaded files)\n",
    "temperature_low_files = [\n",
    "    '/content/MinT-N1-2003-1998.csv',\n",
    "    '/content/MinT-N1-2013-2004.csv',\n",
    "    '/content/MinT-N1-2023-2014.csv',\n",
    "    '/content/MinT-N2-2003-1998.csv',\n",
    "    '/content/MinT-N2-2013-2004.csv',\n",
    "    '/content/MinT-N2-2023-2014.csv'\n",
    "]\n",
    "\n",
    "# Create an empty DataFrame to store the concatenated data\n",
    "temperature_low_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over the file paths and concatenate the DataFrames\n",
    "for file_path in temperature_low_files:\n",
    "    temp_df = pd.read_csv(file_path)  # Read each CSV file\n",
    "    temperature_low_data = pd.concat([temperature_low_data, temp_df], ignore_index=True)  # Concatenate to the main DataFrame\n",
    "\n",
    "# Display the first 10 rows of the concatenated DataFrame\n",
    "temperature_low_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CrLFVfOUgkqy",
    "outputId": "07f283f8-2477-47fd-e7f2-1e05f9c74a7c"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบค่าว่างใน DataFrame\n",
    "null_values = temperature_low_data.isnull()\n",
    "\n",
    "# แสดงผลลัพธ์การตรวจสอบค่าที่หายไป\n",
    "null_values.head(10)  # แสดง 10 แถวแรก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdIFcE6Ogkqy",
    "outputId": "66103389-87b3-4450-c644-dd28f2814bdb"
   },
   "outputs": [],
   "source": [
    "# นับจำนวนค่าที่หายไปในแต่ละคอลัมน์และแสดงผลลัพธ์\n",
    "print(temperature_low_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4S6cRA-gkqz",
    "outputId": "f36b95d0-9b97-4382-d5fd-2060a7d5383a"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_temperature_low = temperature_low_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_temperature_low.head(10)\n",
    "\n",
    "# แสดงขนาดของ DataFrame ใหม่\n",
    "cleaned_temperature_low.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pNmCGE7agkqz",
    "outputId": "f852da03-72e2-4bda-86ad-b85b1b4f3a16"
   },
   "outputs": [],
   "source": [
    "# ลบแถวที่มีค่าที่หายไปและเก็บไว้ใน DataFrame ใหม่\n",
    "cleaned_temperature_low = temperature_low_data.dropna()\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame ใหม่\n",
    "cleaned_temperature_low.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "o2UFR3kvgkqz",
    "outputId": "4753caec-a35a-4589-df4a-9926abc8ac3d"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKjHLz7agkqz",
    "outputId": "0fbe6401-125a-4607-82ab-0f5f8973f663"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dthC8CDIgkq0",
    "outputId": "fcb4c690-ee4b-4341-ac90-1975a2fbd553"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95xadpRVgkq0",
    "outputId": "f9a220d3-ca56-48e6-fa3a-288991e681c6"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบจำนวนค่าที่หายไปในแต่ละคอลัมน์ของ DataFrame ที่ทำความสะอาดแล้ว\n",
    "missing_values_summary = cleaned_temperature_low.isnull().sum()\n",
    "\n",
    "# แสดงผลลัพธ์ของจำนวนค่าที่หายไปในแต่ละคอลัมน์\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "gkVRyoVLgkq0",
    "outputId": "0f9d2119-3c18-4655-9a90-0b5691622495"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์ใน DataFrame\n",
    "cleaned_temperature_low.rename(\n",
    "    columns={\n",
    "        \"ปริมาณฝน(มิลลิเมตร)\": \"NO\",\n",
    "        \"Unnamed: 1\": \"STATION\",\n",
    "        \"Unnamed: 2\": \"Datetime\",\n",
    "        \"Unnamed: 3\": \"Day1\",\n",
    "        \"Unnamed: 4\": \"Day2\",\n",
    "        \"Unnamed: 5\": \"Day3\",\n",
    "        \"Unnamed: 6\": \"Day4\",\n",
    "        \"Unnamed: 7\": \"Day5\",\n",
    "        \"Unnamed: 8\": \"Day6\",\n",
    "        \"Unnamed: 9\": \"Day7\",\n",
    "        \"Unnamed: 10\": \"Day8\",\n",
    "        \"Unnamed: 11\": \"Day9\",\n",
    "        \"Unnamed: 12\": \"Day10\",\n",
    "        \"Unnamed: 13\": \"Day11\",\n",
    "        \"Unnamed: 14\": \"Day12\",\n",
    "        \"Unnamed: 15\": \"Day13\",\n",
    "        \"Unnamed: 16\": \"Day14\",\n",
    "        \"Unnamed: 17\": \"Day15\",\n",
    "        \"Unnamed: 18\": \"Day16\",\n",
    "        \"Unnamed: 19\": \"Day17\",\n",
    "        \"Unnamed: 20\": \"Day18\",\n",
    "        \"Unnamed: 21\": \"Day19\",\n",
    "        \"Unnamed: 22\": \"Day20\",\n",
    "        \"Unnamed: 23\": \"Day21\",\n",
    "        \"Unnamed: 24\": \"Day22\",\n",
    "        \"Unnamed: 25\": \"Day23\",\n",
    "        \"Unnamed: 26\": \"Day24\",\n",
    "        \"Unnamed: 27\": \"Day25\",\n",
    "        \"Unnamed: 28\": \"Day26\",\n",
    "        \"Unnamed: 29\": \"Day27\",\n",
    "        \"Unnamed: 30\": \"Day28\",\n",
    "        \"Unnamed: 31\": \"Day29\",\n",
    "        \"Unnamed: 32\": \"Day30\",\n",
    "        \"Unnamed: 33\": \"Day31\",\n",
    "        \"Unnamed: 34\": \"average\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# แสดง DataFrame หลังจากเปลี่ยนชื่อคอลัมน์\n",
    "cleaned_temperature_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr8Tk3_Fgkq0",
    "outputId": "8ece7dbd-044c-439c-e915-7e1655f36604"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_low['Datetime'] = pd.to_datetime(cleaned_temperature_low['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_low['Year'] = cleaned_temperature_low['Datetime'].dt.year\n",
    "cleaned_temperature_low['Month'] = cleaned_temperature_low['Datetime'].dt.month\n",
    "\n",
    "# ตรวจสอบปีที่หายไปในแต่ละสถานี\n",
    "missing_years_list = []\n",
    "for station in cleaned_temperature_low['STATION'].unique():\n",
    "    # สร้างชุดปีทั้งหมดที่มีในข้อมูล\n",
    "    all_years = set(cleaned_temperature_low['Year'])\n",
    "    years_with_data = set(cleaned_temperature_low[cleaned_temperature_low['STATION'] == station]['Year'])\n",
    "    missing_years_in_station = all_years - years_with_data\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในแต่ละปีที่มีข้อมูล\n",
    "    for year in missing_years_in_station:\n",
    "        missing_years_list.append({\n",
    "            'STATION': station,\n",
    "            'Year': year,\n",
    "            'Missing Months': 'All months missing'\n",
    "        })\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = cleaned_temperature_low[(cleaned_temperature_low['STATION'] == station) & (cleaned_temperature_low['Year'] == year)]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        if missing_months:\n",
    "            missing_years_list.append({\n",
    "                'STATION': station,\n",
    "                'Year': year,\n",
    "                'Missing Months': sorted(missing_months)\n",
    "            })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_years_df = pd.DataFrame(missing_years_list)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_years_df.empty:\n",
    "    print(\"มีสถานีที่มีข้อมูลปีและเดือนหายไป:\")\n",
    "    print(missing_years_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลปีและเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVOj2JAn_n7S",
    "outputId": "fa1528f8-492c-4740-edec-0bfb45879763"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_low['Datetime'] = pd.to_datetime(cleaned_temperature_low['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_low['Year'] = cleaned_temperature_low['Datetime'].dt.year\n",
    "cleaned_temperature_low['Month'] = cleaned_temperature_low['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "missing_months_summary = []\n",
    "\n",
    "for station in cleaned_temperature_low['STATION'].unique():\n",
    "    station_data = cleaned_temperature_low[cleaned_temperature_low['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "    missing_percentage = (missing_months_count / total_months) * 100\n",
    "\n",
    "    # เก็บข้อมูลผลลัพธ์\n",
    "    missing_months_summary.append({\n",
    "        'STATION': station,\n",
    "        'Total Missing Months': missing_months_count,\n",
    "        'Percentage Missing': missing_percentage\n",
    "    })\n",
    "\n",
    "# สร้าง DataFrame สำหรับข้อมูลที่หายไป\n",
    "missing_months_df = pd.DataFrame(missing_months_summary)\n",
    "\n",
    "# แสดงข้อมูลที่หายไป\n",
    "if not missing_months_df.empty:\n",
    "    print(\"ข้อมูลเปอร์เซ็นต์ของเดือนที่หายไปสำหรับแต่ละสถานี:\")\n",
    "    print(missing_months_df)\n",
    "else:\n",
    "    print(\"ทุกสถานีมีข้อมูลเดือนครบถ้วน\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yg38GPBnAMJJ",
    "outputId": "5d8ea938-56fe-4b3c-8e67-77922e2b5f51"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# เปลี่ยนคอลัมน์ 'Datetime' เป็นประเภทวันที่\n",
    "cleaned_temperature_low['Datetime'] = pd.to_datetime(cleaned_temperature_low['Datetime'], format='%b-%y')\n",
    "\n",
    "# สร้างคอลัมน์ใหม่สำหรับปีและเดือน\n",
    "cleaned_temperature_low['Year'] = cleaned_temperature_low['Datetime'].dt.year\n",
    "cleaned_temperature_low['Month'] = cleaned_temperature_low['Datetime'].dt.month\n",
    "\n",
    "# ช่วงปีที่เราต้องการตรวจสอบ\n",
    "start_year = 1998\n",
    "end_year = 2023\n",
    "\n",
    "# สร้าง DataFrame สำหรับการเก็บข้อมูลที่หายไป\n",
    "total_months_all_stations = 0\n",
    "missing_months_all_stations = 0\n",
    "\n",
    "for station in cleaned_temperature_low['STATION'].unique():\n",
    "    station_data = cleaned_temperature_low[cleaned_temperature_low['STATION'] == station]\n",
    "\n",
    "    # สร้างชุดปีทั้งหมดที่คาดหวัง\n",
    "    expected_years = set(range(start_year, end_year + 1))\n",
    "    years_with_data = set(station_data['Year'])\n",
    "    missing_years_in_station = expected_years - years_with_data\n",
    "\n",
    "    total_months = len(expected_years) * 12  # จำนวนเดือนทั้งหมดที่คาดหวัง\n",
    "    missing_months_count = 0\n",
    "\n",
    "    # ตรวจสอบปีที่หายไป\n",
    "    for year in missing_years_in_station:\n",
    "        missing_months_count += 12  # ปีที่หายไปทั้งหมด 12 เดือน\n",
    "\n",
    "    # ตรวจสอบเดือนที่หายไปในปีที่มีข้อมูล\n",
    "    for year in years_with_data:\n",
    "        year_data = station_data[station_data['Year'] == year]\n",
    "        months_in_year = year_data['Month'].unique()\n",
    "        all_months = set(range(1, 13))  # เดือน 1 ถึง 12\n",
    "        missing_months = all_months - set(months_in_year)\n",
    "\n",
    "        missing_months_count += len(missing_months)\n",
    "\n",
    "    # อัปเดตค่าทั้งหมดสำหรับทุกสถานี\n",
    "    total_months_all_stations += total_months\n",
    "    missing_months_all_stations += missing_months_count\n",
    "\n",
    "# คำนวณเปอร์เซ็นต์ของเดือนที่หายไป\n",
    "missing_percentage_all_stations = (missing_months_all_stations / total_months_all_stations) * 100\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(f\"รวมข้อมูลสำหรับทุกสถานีในช่วงปี {start_year}-{end_year}:\")\n",
    "print(f\"จำนวนเดือนทั้งหมดที่คาดหวัง: {total_months_all_stations}\")\n",
    "print(f\"จำนวนเดือนที่หายไป: {missing_months_all_stations}\")\n",
    "print(f\"เปอร์เซ็นต์ของเดือนที่หายไป: {missing_percentage_all_stations:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kVvKzsyugkq1",
    "outputId": "a803f374-2d3b-488c-b7aa-da132b3fc358"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนคอลัมน์ 'Datetime' ใน cleaned_rainfall_data เป็นประเภทวันที่\n",
    "cleaned_temperature_low['Datetime'] = pd.to_datetime(cleaned_temperature_low['Datetime'], format='%b-%y')\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_low.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PuQNqdmhgkq1",
    "outputId": "3d93e8ae-1624-4ca4-b551-4300e627ac4b"
   },
   "outputs": [],
   "source": [
    "# สร้างคอลัมน์ 'YEAR' จากคอลัมน์ 'Datetime'\n",
    "cleaned_temperature_low['YEAR'] = cleaned_temperature_low['Datetime'].dt.year\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_low.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zCOuYh5Bgkq1",
    "outputId": "fe0a620e-95e9-454a-8c31-9f2f616c6568"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนชื่อคอลัมน์เป็นตัวพิมพ์ใหญ่\n",
    "cleaned_temperature_low.columns = [col.upper() for col in cleaned_temperature_low.columns]\n",
    "\n",
    "# กำหนดชื่อคอลัมน์ใหม่ (ตัวพิมพ์ใหญ่)\n",
    "new_columns = [\n",
    "    'STATION', 'DATETIME',\n",
    "    'DAY1', 'DAY2', 'DAY3', 'DAY4', 'DAY5', 'DAY6', 'DAY7', 'DAY8', 'DAY9',\n",
    "    'DAY10', 'DAY11', 'DAY12', 'DAY13', 'DAY14', 'DAY15', 'DAY16', 'DAY17',\n",
    "    'DAY18', 'DAY19', 'DAY20', 'DAY21', 'DAY22', 'DAY23', 'DAY24', 'DAY25',\n",
    "    'DAY26', 'DAY27', 'DAY28', 'DAY29', 'DAY30', 'DAY31', 'AVERAGE'\n",
    "]\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่ต้องการใน DataFrame\n",
    "cleaned_temperature_low = cleaned_temperature_low[new_columns]\n",
    "\n",
    "# สร้างคอลัมน์ 'NO' และเพิ่ม \"NO\" ในทุกแถว\n",
    "cleaned_temperature_low['NO'] = 'NO'\n",
    "\n",
    "# จัดเรียงคอลัมน์ใหม่ให้อยู่ในลำดับที่ต้องการ\n",
    "cleaned_temperature_low = cleaned_temperature_low[['NO'] + new_columns]\n",
    "\n",
    "# แสดง 10 แถวแรกของ DataFrame\n",
    "cleaned_temperature_low.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUVArKD-gkq2",
    "outputId": "2e3cbf6c-78a5-40e9-86e3-8aed5b7c311d"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yAIxDuw9gkq2",
    "outputId": "5cf94ec3-2a49-489e-cffe-211ec12fdbd0"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low['NO'] = range(1, len(cleaned_temperature_low) + 1)\n",
    "cleaned_temperature_low.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jKdMLAPEY684",
    "outputId": "a450c64d-c06e-4eb7-a0b3-3b48847d5298"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.drop(columns=['NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaet_7cLdnIt",
    "outputId": "6e4bb977-29f6-4517-da6f-7d6536ec6574"
   },
   "outputs": [],
   "source": [
    "cleaned_temperature_low.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iv8qpF4j_go",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KljRmaH3GDVY",
    "outputId": "dd9a558e-1511-4ace-8e6f-ddb73a036601"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# โหลดข้อมูลทั้งสอง DataFrame (แทนที่ด้วยข้อมูลของคุณ)\n",
    "temperature_high = cleaned_temperature_data # ข้อมูลอุณหภูมิสูง\n",
    "temperature_low = cleaned_temperature_low # ข้อมูลอุณหภูมิต่ำ\n",
    "\n",
    "# รีเซ็ตดัชนีทั้งสอง DataFrame\n",
    "temperature_high_reset = temperature_high.reset_index(drop=True)\n",
    "temperature_low_reset = temperature_low.reset_index(drop=True)\n",
    "\n",
    "# ตรวจสอบข้อมูลที่มีใน temperature_high แต่ไม่มีใน temperature_low\n",
    "merged_data = pd.merge(\n",
    "    temperature_high_reset[['STATION', 'DATETIME']],\n",
    "    temperature_low_reset[['STATION', 'DATETIME']],\n",
    "    on=['STATION', 'DATETIME'],\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# แสดงข้อมูลที่อยู่ใน temperature_high แต่ไม่อยู่ใน temperature_low\n",
    "missing_in_temperature_low = merged_data[merged_data['_merge'] == 'left_only']\n",
    "print(\"ข้อมูลที่มีใน temperature_high แต่ไม่มีใน temperature_low:\")\n",
    "print(missing_in_temperature_low)\n",
    "\n",
    "# แสดงข้อมูลที่อยู่ใน temperature_low แต่ไม่อยู่ใน temperature_high\n",
    "missing_in_temperature_high = merged_data[merged_data['_merge'] == 'right_only']\n",
    "print(\"ข้อมูลที่มีใน temperature_low แต่ไม่มีใน temperature_high:\")\n",
    "print(missing_in_temperature_high)\n",
    "\n",
    "# ถ้าต้องการตรวจสอบเฉพาะข้อมูลที่ตรงกันในทั้งสอง DataFrame เพื่อเปรียบเทียบ AVERAGE\n",
    "common_data = pd.merge(\n",
    "    temperature_high_reset,\n",
    "    temperature_low_reset,\n",
    "    on=['STATION', 'DATETIME'],\n",
    "    suffixes=('_temperature_high', '_temperature_low')\n",
    ")\n",
    "\n",
    "# # หาค่า AVERAGE ที่ไม่ตรงกัน\n",
    "# mismatched_rows = common_data[common_data['AVERAGE_temperature_high'] != common_data['AVERAGE_temperature_low']]\n",
    "\n",
    "# # แสดงผลข้อมูลที่แตกต่างกัน\n",
    "# if not mismatched_rows.empty:\n",
    "#     print(\"แถวที่มีข้อมูลไม่ตรงกันในคอลัมน์ AVERAGE:\")\n",
    "#     print(mismatched_rows[['STATION', 'DATETIME', 'AVERAGE_temperature_high', 'AVERAGE_temperature_low']])\n",
    "# else:\n",
    "#     print(\"ข้อมูลในคอลัมน์ AVERAGE ตรงกันทุกแถว\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "oNKukQ7NsIul",
    "outputId": "f3a036f8-b11b-460a-fba0-da6bfc421cba"
   },
   "outputs": [],
   "source": [
    "temperature_high_reset.drop(columns=['NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8QElBloXs-tL",
    "outputId": "e52bfaa9-7eed-440d-951c-1915e72209c3"
   },
   "outputs": [],
   "source": [
    "temperature_low_reset.drop(columns=['NO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Tc5uxWj20GgF",
    "outputId": "c8149957-ae01-4a2c-b9f6-e5bf9c4104e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ตรวจสอบความยาวของ DataFrame\n",
    "print(f\"Length of temperature_high: {len(temperature_high)}\")\n",
    "print(f\"Length of temperature_low: {len(temperature_low)}\")\n",
    "\n",
    "# ตรวจสอบคอลัมน์ 'AVERAGE'\n",
    "assert 'AVERAGE' in temperature_high.columns, \"Column 'AVERAGE' not found in temperature_high data.\"\n",
    "assert 'AVERAGE' in temperature_low.columns, \"Column 'AVERAGE' not found in temperature_low data.\"\n",
    "\n",
    "# ทำการรวมข้อมูลที่ไม่ตรงกัน\n",
    "common_dates = temperature_high['DATETIME'].isin(temperature_low['DATETIME'])\n",
    "temperature_high = temperature_high[common_dates]\n",
    "temperature_low = temperature_low[temperature_low['DATETIME'].isin(temperature_high['DATETIME'])]\n",
    "\n",
    "# ตรวจสอบอีกครั้งว่าจำนวนแถวตรงกันแล้ว\n",
    "print(f\"Updated length of temperature_high: {len(temperature_high)}\")\n",
    "print(f\"Updated length of temperature_low: {len(temperature_low)}\")\n",
    "\n",
    "# คำนวณค่าอุณหภูมิรายเดือน\n",
    "def calculate_monthly_avg(high_df, low_df):\n",
    "    # คำนวณค่าอุณหภูมิเฉลี่ยรายเดือนจากข้อมูลสูงสุดและต่ำสุด\n",
    "    # Convert 'AVERAGE' columns to numeric type\n",
    "    high_avg = pd.to_numeric(high_df['AVERAGE'], errors='coerce') # Convert to numeric, replace invalid values with NaN\n",
    "    low_avg = pd.to_numeric(low_df['AVERAGE'], errors='coerce') # Convert to numeric, replace invalid values with NaN\n",
    "\n",
    "    # คำนวณอุณหภูมิรายเดือน\n",
    "    monthly_avg = (high_avg + low_avg) / 2\n",
    "\n",
    "    # สร้าง DataFrame ใหม่สำหรับเก็บข้อมูลอุณหภูมิรายเดือน\n",
    "    Tem = high_df[['STATION', 'DATETIME']].copy()\n",
    "    Tem['MONTHLY_AVERAGE'] = monthly_avg\n",
    "\n",
    "    return Tem\n",
    "\n",
    "# คำนวณค่าอุณหภูมิเฉลี่ยรายเดือน\n",
    "Tem = calculate_monthly_avg(temperature_high, temperature_low)\n",
    "\n",
    "Tem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_5LEnPAsO_k",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Q1WxSos4sQ4w",
    "outputId": "bfc09773-8d73-4229-ab9e-9b3b90ef41fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ตรวจสอบและจัดการค่าที่เป็น 'T' หรือ '-'\n",
    "cleaned_rainfall_data['RAINFALL'] = cleaned_rainfall_data['RAINFALL'].replace(['T', '-'], np.nan).astype(float)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยและค่าเบี่ยงเบนมาตรฐานของปริมาณน้ำฝนที่ไม่เป็น NaN\n",
    "mean_rainfall = cleaned_rainfall_data['RAINFALL'].mean()\n",
    "std_rainfall = cleaned_rainfall_data['RAINFALL'].std()\n",
    "\n",
    "# สร้างฟังก์ชันสำหรับคำนวณ SPI\n",
    "def calculate_spi(rainfall, mean, std):\n",
    "    if np.isnan(rainfall):  # ถ้าค่า Rainfall เป็น NaN (เช่น 'T' หรือ '-')\n",
    "        return np.nan  # ใช้ NaN แทน '-'\n",
    "    if std == 0:  # ถ้า std เป็น 0 ให้คืนค่า NaN\n",
    "        return np.nan\n",
    "    else:\n",
    "        # คำนวณค่า Z-score สำหรับ SPI\n",
    "        return (rainfall - mean) / std\n",
    "\n",
    "# คำนวณค่า SPI และเพิ่มเป็นคอลัมน์ใหม่ใน DataFrame\n",
    "cleaned_rainfall_data['SPI'] = cleaned_rainfall_data['RAINFALL'].apply(calculate_spi, args=(mean_rainfall, std_rainfall))\n",
    "\n",
    "# เก็บค่า SPI ออกมาในตัวแปร all_spi\n",
    "all_spi = cleaned_rainfall_data[['STATION', 'DATETIME', 'SPI']].copy()\n",
    "\n",
    "all_spi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IoJQSAiLXle",
    "outputId": "b5cb2900-1fa5-4417-b08e-69e3f1c05cc9"
   },
   "outputs": [],
   "source": [
    "all_spi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yDFbF_yIAKh8",
    "outputId": "150fa1e2-5ceb-42ba-dab9-d04306a11a16"
   },
   "outputs": [],
   "source": [
    "all_spi.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IR-_i-5aAL_h",
    "outputId": "72a61357-d17d-4f2c-d625-4196236d57f6"
   },
   "outputs": [],
   "source": [
    "all_spi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZqBIWp1RLWc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IuSW7Xc49CjE",
    "outputId": "fe1b2659-45f6-4d6d-e921-8678f148facb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# อ่านข้อมูล\n",
    "rainfall_data = cleaned_rainfall_data\n",
    "evaporation_data = cleaned_evaporation_data\n",
    "temperature_data = Tem\n",
    "\n",
    "# ฟังก์ชันคำนวณค่า PET โดยใช้ Thornthwaite (ต้องการอุณหภูมิรายเดือน)\n",
    "def thornthwaite_pet(temp_mean):\n",
    "    # คำนวณค่า I (thermal index)\n",
    "    I = np.sum((temp_mean / 5) ** 1.514)\n",
    "\n",
    "    # คำนวณค่า a โดยใช้สูตร Thornthwaite\n",
    "    a = 6.75e-7 * I**3 - 7.71e-5 * I**2 + 1.792e-2 * I + 0.49239\n",
    "\n",
    "    # คำนวณค่า PET โดยใช้สูตร Thornthwaite\n",
    "    pet = 16 * ((10 * temp_mean / I) ** a)\n",
    "    return pet\n",
    "\n",
    "# ตรวจสอบและจัดการค่า NaN ในข้อมูล\n",
    "def preprocess_data(df):\n",
    "    df = df.replace(['T', '-'], np.nan).astype(float)\n",
    "    return df\n",
    "\n",
    "# ประมวลผลข้อมูล\n",
    "rainfall_data['RAINFALL'] = preprocess_data(rainfall_data['RAINFALL'])\n",
    "evaporation_data['EVAPORATION'] = preprocess_data(evaporation_data['EVAPORATION'])\n",
    "temperature_data['MONTHLY_AVERAGE'] = preprocess_data(temperature_data['MONTHLY_AVERAGE'])\n",
    "\n",
    "# หาสถานีที่ไม่ซ้ำกัน\n",
    "stations = rainfall_data['STATION'].unique()\n",
    "\n",
    "# สร้าง DataFrame ว่างเพื่อเก็บผลลัพธ์\n",
    "all_spei_results = pd.DataFrame()\n",
    "\n",
    "# วนลูปผ่านแต่ละสถานี\n",
    "for station in stations:\n",
    "    # กรองข้อมูลสำหรับสถานีปัจจุบัน\n",
    "    station_rainfall = rainfall_data[rainfall_data['STATION'] == station]\n",
    "    station_temperature = temperature_data[temperature_data['STATION'] == station]\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลทั้งสองประเภท\n",
    "    if station_rainfall.empty or station_temperature.empty:\n",
    "        print(f\"Warning: No data for station {station}\")\n",
    "        continue\n",
    "\n",
    "    # คำนวณค่า PET โดยใช้ข้อมูลอุณหภูมิรายเดือน\n",
    "    station_temperature['PET'] = station_temperature['MONTHLY_AVERAGE'].apply(thornthwaite_pet)\n",
    "\n",
    "    # รวมข้อมูลการฝนตกและ PET\n",
    "    combined_data = station_rainfall.merge(station_temperature[['DATETIME', 'PET']], on='DATETIME', how='inner')\n",
    "\n",
    "    # คำนวณดัชนีความชื้น\n",
    "    combined_data['W'] = combined_data['RAINFALL'] - combined_data['PET']\n",
    "\n",
    "    # คำนวณค่าเฉลี่ยและค่าเบี่ยงเบนมาตรฐานของดัชนีความชื้น\n",
    "    mean_w = combined_data['W'].mean()\n",
    "    std_w = combined_data['W'].std()\n",
    "\n",
    "    # คำนวณค่า SPEI\n",
    "    combined_data['SPEI'] = (combined_data['W'] - mean_w) / std_w\n",
    "\n",
    "    # เลือกเฉพาะคอลัมน์ที่ต้องการ\n",
    "    selected_columns = combined_data[['STATION', 'DATETIME', 'PET', 'W', 'SPEI']]\n",
    "\n",
    "    # เพิ่มผลลัพธ์ลงใน DataFrame all_spei_results\n",
    "    all_spei_results = pd.concat([all_spei_results, selected_columns], ignore_index=True)\n",
    "\n",
    "# แสดงผลลัพธ์สุดท้าย\n",
    "all_spei_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w-GTzglTaZj",
    "outputId": "da3624bb-15ca-43ff-f8a4-4e6e1eb97812"
   },
   "outputs": [],
   "source": [
    "all_spei_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dlVIxUvVIHer",
    "outputId": "061b6c73-70ba-4b8b-ea0d-5be0a44b8707"
   },
   "outputs": [],
   "source": [
    "all_spei_results.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiMmCiwqKVls",
    "outputId": "b43a6869-747c-4d99-932e-140bba37c17e"
   },
   "outputs": [],
   "source": [
    "all_spei_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7pL4ndUDhNW",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Analysis data SPI and SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6cgpZ9tFCxzn",
    "outputId": "261de7db-0a2a-48aa-82ef-a1896bf60caa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ตรวจสอบคอลัมน์ที่เป็นตัวเลข\n",
    "spi_numeric = all_spi.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# เลือกเฉพาะคอลัมน์ที่เป็นค่า SPEI เท่านั้น\n",
    "spei_numeric = all_spei_results[['SPEI']]  # แทนที่ 'SPEI' ด้วยชื่อคอลัมน์จริงของ SPEI ถ้าต่างกัน\n",
    "\n",
    "# คำนวณค่าสถิติพื้นฐานสำหรับ SPI\n",
    "spi_mean = spi_numeric.mean()\n",
    "spi_std = spi_numeric.std()\n",
    "spi_min = spi_numeric.min()\n",
    "spi_max = spi_numeric.max()\n",
    "spi_percentiles = spi_numeric.quantile([0.25, 0.50, 0.75])\n",
    "\n",
    "# คำนวณค่าสถิติพื้นฐานสำหรับ SPEI\n",
    "spei_mean = spei_numeric.mean()\n",
    "spei_std = spei_numeric.std()\n",
    "spei_min = spei_numeric.min()\n",
    "spei_max = spei_numeric.max()\n",
    "spei_percentiles = spei_numeric.quantile([0.25, 0.50, 0.75])\n",
    "\n",
    "# แสดงผลลัพธ์\n",
    "print(\"SPI Statistics:\")\n",
    "print(\"Mean:\\n\", spi_mean)\n",
    "print(\"Standard Deviation:\\n\", spi_std)\n",
    "print(\"Minimum:\\n\", spi_min)\n",
    "print(\"Maximum:\\n\", spi_max)\n",
    "print(\"Percentiles:\\n\", spi_percentiles)\n",
    "\n",
    "print(\"\\nSPEI Statistics:\")\n",
    "print(\"Mean:\\n\", spei_mean)\n",
    "print(\"Standard Deviation:\\n\", spei_std)\n",
    "print(\"Minimum:\\n\", spei_min)\n",
    "print(\"Maximum:\\n\", spei_max)\n",
    "print(\"Percentiles:\\n\", spei_percentiles)\n",
    "\n",
    "# สร้าง Histogram และ Box Plot สำหรับ SPI\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(spi_numeric, kde=True)\n",
    "plt.title('Distribution of SPI')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=spi_numeric)\n",
    "plt.title('Box Plot of SPI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# สร้าง Histogram และ Box Plot สำหรับ SPEI\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(spei_numeric, kde=True)\n",
    "plt.title('Distribution of SPEI')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=spei_numeric)\n",
    "plt.title('Box Plot of SPEI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BPfiBxP_N4Kp",
    "outputId": "f77b3879-ae07-47f0-8568-8ffa80315d03"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# รวมข้อมูล SPI และ SPEI เข้าด้วยกันโดยใช้ 'STATION' และ 'DATETIME' เป็นตัวเชื่อม\n",
    "merged_data = pd.merge(all_spi, all_spei_results, on=['STATION', 'DATETIME'])\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' ให้เป็นรูปแบบ datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# ลูปผ่านแต่ละสถานี\n",
    "stations = merged_data['STATION'].unique()  # ดึงชื่อสถานีทั้งหมดโดยไม่ซ้ำ\n",
    "\n",
    "for station in stations:\n",
    "    # เลือกข้อมูลสำหรับแต่ละสถานี\n",
    "    station_data = merged_data[merged_data['STATION'] == station]\n",
    "\n",
    "    # สร้างกราฟ SPI และ SPEI สำหรับสถานีที่เลือก\n",
    "    plt.figure(figsize=(14, 7))  # ขยายกราฟให้กว้างขึ้น\n",
    "    plt.plot(station_data['DATETIME'], station_data['SPI'], label='SPI', color='blue')\n",
    "    plt.plot(station_data['DATETIME'], station_data['SPEI'], label='SPEI', color='red')\n",
    "\n",
    "    # ตั้งค่าแกน Y ให้อยู่ในช่วง -4 ถึง 4\n",
    "    plt.ylim(-4, 4)\n",
    "\n",
    "    # กำหนดชื่อกราฟและแกน\n",
    "    plt.title(f'SPI and SPEI over Time for {station}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index Value')\n",
    "    plt.legend()\n",
    "\n",
    "    # ตั้งค่าแกน X ให้แสดงปี\n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()  # หมุนวันที่ให้พอดีกับกราฟ\n",
    "\n",
    "    # ตั้งค่าแกน Y ให้แสดงค่าในช่วง -4 ถึง 4\n",
    "    plt.yticks(range(-4, 5))\n",
    "\n",
    "    # แสดงกราฟของแต่ละสถานีแยกกัน\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7WV4T4csPJDt",
    "outputId": "2bf15ab0-6932-4714-9277-a3f4480dbfd8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# รวมข้อมูล SPI และ SPEI เข้าด้วยกันโดยใช้ 'STATION' และ 'DATETIME' เป็นตัวเชื่อม\n",
    "merged_data = pd.merge(all_spi, all_spei_results, on=['STATION', 'DATETIME'])\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' ให้เป็นรูปแบบ datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# ลูปผ่านแต่ละสถานี\n",
    "stations = merged_data['STATION'].unique()  # ดึงชื่อสถานีทั้งหมดโดยไม่ซ้ำ\n",
    "\n",
    "for station in stations:\n",
    "    # เลือกข้อมูลสำหรับแต่ละสถานี\n",
    "    station_data = merged_data[merged_data['STATION'] == station]\n",
    "\n",
    "    # กราฟฮิสโตแกรมของ SPI และ SPEI\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # ฮิสโตแกรมของ SPI\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(station_data['SPI'].dropna(), bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'SPI Distribution for {station}')\n",
    "    plt.xlabel('SPI Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # ฮิสโตแกรมของ SPEI\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(station_data['SPEI'].dropna(), bins=20, color='red', alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'SPEI Distribution for {station}')\n",
    "    plt.xlabel('SPEI Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # กราฟกล่องของ SPI และ SPEI\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # กราฟกล่องของ SPI\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(y=station_data['SPI'], color='blue')\n",
    "    plt.title(f'SPI Boxplot for {station}')\n",
    "    plt.ylabel('SPI Value')\n",
    "\n",
    "    # กราฟกล่องของ SPEI\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=station_data['SPEI'], color='red')\n",
    "    plt.title(f'SPEI Boxplot for {station}')\n",
    "    plt.ylabel('SPEI Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ytdF5XvARRnf",
    "outputId": "443ff187-8fe7-471d-a43f-575e45a37082"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# รวมข้อมูล SPI และ SPEI เข้าด้วยกันโดยใช้ 'STATION' และ 'DATETIME' เป็นตัวเชื่อม\n",
    "merged_data = pd.merge(all_spi, all_spei_results, on=['STATION', 'DATETIME'])\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' ให้เป็นรูปแบบ datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# ลูปผ่านแต่ละสถานี\n",
    "stations = merged_data['STATION'].unique()  # ดึงชื่อสถานีทั้งหมดโดยไม่ซ้ำ\n",
    "\n",
    "for station in stations:\n",
    "    # เลือกข้อมูลสำหรับแต่ละสถานี\n",
    "    station_data = merged_data[merged_data['STATION'] == station]\n",
    "\n",
    "    # ตั้งค่า 'DATETIME' เป็น Index และกรองข้อมูลในรูปแบบ Pivot Table\n",
    "    station_data.set_index('DATETIME', inplace=True)\n",
    "\n",
    "    # สร้างข้อมูลสำหรับ heatmap โดยการแปลงค่า 'DATETIME' เป็นปีและเดือน\n",
    "    spi_heatmap_data = station_data[['SPI']].resample('M').mean()\n",
    "    spi_heatmap_data['Year'] = spi_heatmap_data.index.year\n",
    "    spi_heatmap_data['Month'] = spi_heatmap_data.index.month\n",
    "\n",
    "    spei_heatmap_data = station_data[['SPEI']].resample('M').mean()\n",
    "    spei_heatmap_data['Year'] = spei_heatmap_data.index.year\n",
    "    spei_heatmap_data['Month'] = spei_heatmap_data.index.month\n",
    "\n",
    "    # สร้าง Pivot Table สำหรับ SPI\n",
    "    spi_pivot_table = spi_heatmap_data.pivot_table(index='Month', columns='Year', values='SPI')\n",
    "\n",
    "    # สร้าง Pivot Table สำหรับ SPEI\n",
    "    spei_pivot_table = spei_heatmap_data.pivot_table(index='Month', columns='Year', values='SPEI')\n",
    "\n",
    "    # สร้าง heatmap สำหรับ SPI\n",
    "    plt.figure(figsize=(18, 10))  # ขยายขนาดกราฟ\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(spi_pivot_table, cmap='coolwarm', annot=False, cbar=True, fmt=\".1f\", linewidths=0.5)\n",
    "    plt.title(f'SPI Heatmap for {station}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Month')\n",
    "    plt.xticks(ticks=range(len(spi_pivot_table.columns)), labels=spi_pivot_table.columns, rotation=45)\n",
    "    plt.yticks(ticks=range(len(spi_pivot_table.index)), labels=spi_pivot_table.index, rotation=0)\n",
    "\n",
    "    # สร้าง heatmap สำหรับ SPEI\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(spei_pivot_table, cmap='coolwarm', annot=False, cbar=True, fmt=\".1f\", linewidths=0.5)\n",
    "    plt.title(f'SPEI Heatmap for {station}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Month')\n",
    "    plt.xticks(ticks=range(len(spei_pivot_table.columns)), labels=spei_pivot_table.columns, rotation=45)\n",
    "    plt.yticks(ticks=range(len(spei_pivot_table.index)), labels=spei_pivot_table.index, rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FAi7fuxzvps"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwwsfMWyRUQw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Xgboost SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fQGvNb5aOvYt",
    "outputId": "a4b664e6-05d9-4d4a-f0d4-4d0d7b619eac"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spei_results[all_spei_results['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spei_results[all_spei_results['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# เปิดไฟล์ CSV เพื่อบันทึกผลลัพธ์รายเดือน\n",
    "with open('predicted_spei_results_Xgboost.csv', mode='w', newline='', encoding='utf-8') as file_monthly, \\\n",
    "     open('predicted_Quarterly_spei_xgboost.csv', mode='w', newline='', encoding='utf-8') as file_quarterly:\n",
    "\n",
    "    writer_monthly = csv.writer(file_monthly)\n",
    "    writer_quarterly = csv.writer(file_quarterly)\n",
    "\n",
    "    # ปรับหัวคอลัมน์ตามที่ต้องการ\n",
    "    writer_monthly.writerow(['STATION', 'DATETIME', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "    writer_quarterly.writerow(['STATION', 'Quarter', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "\n",
    "    for station in stations:\n",
    "        # กรองข้อมูลตามสถานี\n",
    "        train_station = train_data[train_data['STATION'] == station]\n",
    "        test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "        # ทำความสะอาดข้อมูล\n",
    "        train_station = clean_data(train_station)\n",
    "        test_station = clean_data(test_station)\n",
    "\n",
    "        # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "        if test_station.empty:\n",
    "            print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "            continue\n",
    "\n",
    "        # สร้าง features และ target\n",
    "        X_train_station = train_station[['PET', 'W']]\n",
    "        y_train_station = train_station['SPEI']\n",
    "        X_test_station = test_station[['PET', 'W']]\n",
    "        y_test_station = test_station['SPEI']\n",
    "\n",
    "        # สร้าง DMatrix สำหรับ XGBoost\n",
    "        dtrain_station = xgb.DMatrix(X_train_station, label=y_train_station)\n",
    "        dtest_station = xgb.DMatrix(X_test_station, label=y_test_station)\n",
    "\n",
    "        # กำหนดพารามิเตอร์ของโมเดล\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.05,\n",
    "            'n_estimators': 200,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8\n",
    "        }\n",
    "\n",
    "        # ฝึกโมเดล\n",
    "        model = xgb.train(params, dtrain_station, num_boost_round=200)\n",
    "\n",
    "        # ทำนายผลสำหรับปี 2023\n",
    "        Xgboost_spei_2023 = model.predict(dtest_station)\n",
    "\n",
    "        # ประเมินผล\n",
    "        mse = mean_squared_error(y_test_station, Xgboost_spei_2023)\n",
    "        print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "        # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPEI 2023', marker='o')\n",
    "        plt.plot(test_station['DATETIME'], Xgboost_spei_2023, label='Predicted SPEI 2023', marker='x')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('SPEI')\n",
    "        plt.title(f'Station: {station} - Actual vs Predicted SPEI for 2023')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023 และบันทึกผลลัพธ์รายเดือนลงในไฟล์ CSV\n",
    "        print(f\"\\nPredicted vs Actual SPEI for 2023 at station {station}:\")\n",
    "        for date, prediction, actual in zip(test_station['DATETIME'], Xgboost_spei_2023, y_test_station):\n",
    "            print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "            # บันทึกเฉพาะผลลัพธ์รายเดือน\n",
    "            writer_monthly.writerow([station, date.strftime('%Y-%m-%d'), actual, prediction])\n",
    "\n",
    "        # คำนวณค่าเฉลี่ย SPEI รายไตรมาส\n",
    "        test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "        test_station['Xgboost_Predicted_SPEI'] = Xgboost_spei_2023\n",
    "        Xgboost_Quarterly_spei = test_station.groupby('Quarter').agg({\n",
    "            'SPEI': 'mean',  # Actual SPEI\n",
    "            'Xgboost_Predicted_SPEI': 'mean'  # Predicted SPEI\n",
    "        }).rename(columns={'SPEI': 'Actual_SPEI', 'Xgboost_Predicted_SPEI': 'Predicted_SPEI'})\n",
    "\n",
    "        # บันทึกผลลัพธ์รายไตรมาสลงในไฟล์ CSV\n",
    "        for quarter, row in Xgboost_Quarterly_spei.iterrows():\n",
    "            writer_quarterly.writerow([station, str(quarter), row['Actual_SPEI'], row['Predicted_SPEI']])\n",
    "\n",
    "        # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาส (ไม่บันทึกลงไฟล์ CSV)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        Xgboost_Quarterly_spei[['Actual_SPEI', 'Predicted_SPEI']].plot(kind='bar', position=1, width=0.4, label=['Actual SPEI', 'Predicted SPEI'])\n",
    "        plt.legend()\n",
    "        plt.xlabel('Quarter')\n",
    "        plt.ylabel('Average SPEI')\n",
    "        plt.title(f'Station: {station} - Quarterly Average SPEI for 2023')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # แสดงค่าเฉลี่ย SPEI รายไตรมาส\n",
    "        print(f\"\\nQuarterly Average SPEI for 2023 at station {station}:\")\n",
    "        for quarter, row in Xgboost_Quarterly_spei.iterrows():\n",
    "            print(f\"{quarter}: Actual SPEI: {row['Actual_SPEI']:.4f}, Predicted SPEI: {row['Predicted_SPEI']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4o5KoYbiHdBt",
    "outputId": "1a5a852f-ac62-495b-a3d4-bb2ace42dbb2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spei_results\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_data(last_date, features):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "    for feature in features:\n",
    "        dummy_data[feature] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)[feature].mean())\n",
    "    return dummy_data\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['PET', 'W']]\n",
    "    y_train_station = train_station['SPEI']\n",
    "\n",
    "    # สร้าง DMatrix สำหรับ XGBoost\n",
    "    dtrain_station = xgb.DMatrix(X_train_station, label=y_train_station)\n",
    "\n",
    "    # กำหนดพารามิเตอร์ของโมเดล\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8\n",
    "    }\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model = xgb.train(params, dtrain_station, num_boost_round=200)\n",
    "\n",
    "    # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dummy_2024 = generate_2024_data(last_date, ['PET', 'W'])\n",
    "    dummy_2024['STATION'] = station\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2024\n",
    "    X_pred_2024 = dummy_2024[['PET', 'W']]\n",
    "    dpred_2024 = xgb.DMatrix(X_pred_2024)\n",
    "    Gxboost_spei_2024 = model.predict(dpred_2024)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dummy_2024['DATETIME'], Gxboost_spei_2024, label='Predicted SPEI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Predicted SPEI for 2024')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPEI for 2024 at station {station}:\")\n",
    "    for date, prediction in zip(dummy_2024['DATETIME'], Gxboost_spei_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    dummy_2024['Gxboost_Predicted_SPEI'] = Gxboost_spei_2024\n",
    "    Gxboost_Quarterly_spei_2024 = dummy_2024.groupby('Quarter')['Gxboost_Predicted_SPEI'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    Gxboost_Quarterly_spei_2024.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPEI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPEI for 2024')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPEI for 2024 at station {station}:\")\n",
    "    for quarter, value in Gxboost_Quarterly_spei_2024.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV1RUHvKIxsO"
   },
   "source": [
    "## Xgboost SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RgSXv4xkI5Hs",
    "outputId": "e4c43b96-61d9-48cc-a37f-6221afb57495"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# แยกข้อมูลสำหรับการฝึกและทดสอบ\n",
    "train_data = all_spi[all_spi['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spi[all_spi['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือน\n",
    "predicted_spi_results_Xgboost = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายแบบไตรมาส\n",
    "predicted_Quarterly_spi_results_Xgboost = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['SPI']]  # ใช้เพียงค่า SPI เป็น feature\n",
    "    y_train_station = train_station['SPI']     # ใช้ค่า SPI เป็น target\n",
    "    X_test_station = test_station[['SPI']]     # ใช้เพียงค่า SPI เป็น feature\n",
    "    y_test_station = test_station['SPI']       # ใช้ค่า SPI เป็น target\n",
    "\n",
    "    # สร้าง DMatrix สำหรับ XGBoost\n",
    "    dtrain_station = xgb.DMatrix(X_train_station, label=y_train_station)\n",
    "    dtest_station = xgb.DMatrix(X_test_station, label=y_test_station)\n",
    "\n",
    "    # กำหนดพารามิเตอร์ของโมเดล\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8\n",
    "    }\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model = xgb.train(params, dtrain_station, num_boost_round=200)\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    Gxboost_spi_2023 = model.predict(dtest_station)\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, Gxboost_spi_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for SPI prediction in 2023: {mse}\")\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], Gxboost_spi_2023, label='Predicted SPI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPI for 2023')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], Gxboost_spi_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame สำหรับบันทึกผลรายเดือน\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'].values,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': y_test_station,\n",
    "        'Predicted_SPI': Gxboost_spi_2023\n",
    "    })\n",
    "    predicted_spi_results_Xgboost = pd.concat([predicted_spi_results_Xgboost, station_results], ignore_index=True)\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPI รายไตรมาส\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['Gxboost_Predicted_SPI'] = Gxboost_spi_2023\n",
    "    Gxboost_Quarterly_spi_2023 = test_station.groupby('Quarter').agg({\n",
    "        'SPI': 'mean',  # Actual SPI\n",
    "        'Gxboost_Predicted_SPI': 'mean'  # Predicted SPI\n",
    "    }).rename(columns={'SPI': 'Actual_SPI', 'Gxboost_Predicted_SPI': 'Predicted_SPI'})\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    Gxboost_Quarterly_spi_2023[['Actual_SPI', 'Predicted_SPI']].plot(kind='bar', position=1, width=0.4, label=['Actual SPI', 'Predicted SPI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPI for 2023')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPI for 2023 at station {station}:\")\n",
    "    for quarter, row in Gxboost_Quarterly_spi_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual SPI: {row['Actual_SPI']:.4f}, Predicted SPI: {row['Predicted_SPI']:.4f}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์รายไตรมาสลงใน DataFrame\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': Gxboost_Quarterly_spi_2023.index.astype(str),\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': Gxboost_Quarterly_spi_2023['Actual_SPI'],\n",
    "        'Predicted_SPI': Gxboost_Quarterly_spi_2023['Predicted_SPI']\n",
    "    })\n",
    "    predicted_Quarterly_spi_results_Xgboost = pd.concat([predicted_Quarterly_spi_results_Xgboost, quarterly_results], ignore_index=True)\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spi_results_Xgboost.to_csv('predicted_spi_results_Xgboost.csv', index=False, encoding='utf-8')\n",
    "print(\"Monthly prediction results have been saved to 'predicted_spi_results_Xgboost.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_Quarterly_spi_results_Xgboost.to_csv('predicted_Quarterly_spi_xgboost.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly prediction results have been saved to 'predicted_Quarterly_spi_xgboost.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "b5yv0oe2MS28",
    "outputId": "1f09ea75-4a58-4174-a494-1582be7d4b8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# สมมติว่าคุณได้โหลดข้อมูล SPI เข้ามาแล้วในตัวแปร all_spi\n",
    "# all_spi = pd.read_csv('path_to_spi_data.csv')\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spi\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_data(last_date):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "    dummy_data['SPI'] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)['SPI'].mean())\n",
    "    return dummy_data\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['SPI']]\n",
    "    y_train_station = train_station['SPI']\n",
    "\n",
    "    # สร้าง DMatrix สำหรับ XGBoost\n",
    "    dtrain_station = xgb.DMatrix(X_train_station, label=y_train_station)\n",
    "\n",
    "    # กำหนดพารามิเตอร์ของโมเดล\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 200,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8\n",
    "    }\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model = xgb.train(params, dtrain_station, num_boost_round=200)\n",
    "\n",
    "    # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dummy_2024 = generate_2024_data(last_date)\n",
    "    dummy_2024['STATION'] = station\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2024\n",
    "    X_pred_2024 = dummy_2024[['SPI']]\n",
    "    dpred_2024 = xgb.DMatrix(X_pred_2024)\n",
    "    Gxboost_spi_2024 = model.predict(dpred_2024)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dummy_2024['DATETIME'], Gxboost_spi_2024, label='Predicted SPI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Predicted SPI for 2024')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPI for 2024 at station {station}:\")\n",
    "    for date, prediction in zip(dummy_2024['DATETIME'], Gxboost_spi_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    dummy_2024['Gxboost_Predicted_SPI'] = Gxboost_spi_2024\n",
    "    Gxboost_Quarterly_spi_2024 = dummy_2024.groupby('Quarter')['Gxboost_Predicted_SPI'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    Gxboost_Quarterly_spi_2024.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPI for 2024')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPI for 2024 at station {station}:\")\n",
    "    for quarter, value in Gxboost_Quarterly_spi_2024.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNqYYRt9QHMU"
   },
   "source": [
    "## Decision Tree SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "u2H1YXC3QQe4",
    "outputId": "f05fd399-6096-46c5-e036-03ce0b4048bf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spei_results[all_spei_results['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spei_results[all_spei_results['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือน\n",
    "predicted_spei_results_Decisiontree = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การพยากรณ์รายไตรมาส\n",
    "predicted_Quarterly_spei_Decisiontree = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['PET', 'W']]\n",
    "    y_train_station = train_station['SPEI']\n",
    "    X_test_station = test_station[['PET', 'W']]\n",
    "    y_test_station = test_station['SPEI']\n",
    "\n",
    "    # สร้างและฝึกโมเดล Decision Tree\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    DecisionTree_spei_2023 = model.predict(X_test_station)\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, DecisionTree_spei_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame รายเดือน\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'],\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI': y_test_station,\n",
    "        'Predicted_SPEI': DecisionTree_spei_2023\n",
    "    })\n",
    "    predicted_spei_results_Decisiontree = pd.concat([predicted_spei_results_Decisiontree, station_results], ignore_index=True)\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['DecisionTree_Predicted_SPEI'] = DecisionTree_spei_2023\n",
    "    DecisionTree_Quarterly_spei_2023 = test_station.groupby('Quarter').agg({\n",
    "        'SPEI': 'mean',\n",
    "        'DecisionTree_Predicted_SPEI': 'mean'\n",
    "    }).rename(columns={'SPEI': 'Actual_SPEI', 'DecisionTree_Predicted_SPEI': 'Predicted_SPEI'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์การพยากรณ์รายไตรมาสลงใน DataFrame\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': DecisionTree_Quarterly_spei_2023.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI': DecisionTree_Quarterly_spei_2023['Actual_SPEI'],\n",
    "        'Predicted_SPEI': DecisionTree_Quarterly_spei_2023['Predicted_SPEI']\n",
    "    })\n",
    "    predicted_Quarterly_spei_Decisiontree = pd.concat([predicted_Quarterly_spei_Decisiontree, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    DecisionTree_Quarterly_spei_2023[['Actual_SPEI', 'Predicted_SPEI']].plot(kind='bar', position=1, width=0.4, label=['Actual SPEI', 'Predicted SPEI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPEI for 2023 (Decision Tree)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPEI for 2023 at station {station}:\")\n",
    "    for quarter, row in DecisionTree_Quarterly_spei_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual: {row['Actual_SPEI']:.4f}, Predicted: {row['Predicted_SPEI']:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spei_results_Decisiontree.to_csv('predicted_spei_results_Decisiontree.csv', index=False, encoding='utf-8')\n",
    "print(\"Monthly prediction results have been saved to 'predicted_spei_results_Decisiontree.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_Quarterly_spei_Decisiontree.to_csv('predicted_Quarterly_spei_Decisiontree.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly prediction results have been saved to 'predicted_Quarterly_spei_Decisiontree.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "arGQL1ZZQsA8",
    "outputId": "d1f79200-5a2c-46f4-bfef-5a1d0968cafb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spei_results\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_data(last_date, features):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "    for feature in features:\n",
    "        dummy_data[feature] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)[feature].mean())\n",
    "    return dummy_data\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['PET', 'W']]\n",
    "    y_train_station = train_station['SPEI']\n",
    "\n",
    "    # สร้างและฝึกโมเดล Decision Tree\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dummy_2024 = generate_2024_data(last_date, ['PET', 'W'])\n",
    "    dummy_2024['STATION'] = station\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2024\n",
    "    X_pred_2024 = dummy_2024[['PET', 'W']]\n",
    "    Spei_predictions_2024 = model.predict(X_pred_2024)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dummy_2024['DATETIME'], Spei_predictions_2024, label='Predicted SPEI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Predicted SPEI for 2024 (Decision Tree)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPEI for 2024 at station {station} (Decision Tree):\")\n",
    "    for date, prediction in zip(dummy_2024['DATETIME'], Spei_predictions_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    dummy_2024['Predicted_SPEI'] = Spei_predictions_2024\n",
    "    quarterly_data = dummy_2024.groupby('Quarter')['Predicted_SPEI'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    quarterly_data.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPEI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPEI for 2024 (Decision Tree)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPEI for 2024 at station {station} (Decision Tree):\")\n",
    "    for quarter, value in quarterly_data.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n",
    "\n",
    "    # แสดงความสำคัญของ features\n",
    "    feature_importance = model.feature_importances_\n",
    "    features = ['PET', 'W']\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(features, feature_importance)\n",
    "    plt.title(f'Feature Importance for SPEI Prediction at station {station}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nFeature Importance for SPEI Prediction at station {station}:\")\n",
    "    for feature, importance in zip(features, feature_importance):\n",
    "        print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Dqhrq5TSyh"
   },
   "source": [
    "## Decision Tree SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "FpY12HZ8TYaw",
    "outputId": "59c256e3-088c-4f61-ab69-593030eadf9a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# แยกข้อมูลสำหรับการฝึกและทดสอบ\n",
    "train_data = all_spi[all_spi['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spi[all_spi['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame สำหรับเก็บผลลัพธ์การทำนาย\n",
    "predicted_spi_results_DecisionTree = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "predicted_Quarterly_spi_results = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['SPI']]  # ใช้เพียงค่า SPI เป็น feature\n",
    "    y_train_station = train_station['SPI']     # ใช้ค่า SPI เป็น target\n",
    "    X_test_station = test_station[['SPI']]     # ใช้เพียงค่า SPI เป็น feature\n",
    "    y_test_station = test_station['SPI']       # ใช้ค่า SPI เป็น target\n",
    "\n",
    "    # สร้าง DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    spi_predictions_2023 = model.predict(X_test_station)\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, spi_predictions_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for SPI prediction in 2023: {mse}\")\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], spi_predictions_2023, label='Predicted SPI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPI for 2023')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], spi_predictions_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame สำหรับบันทึกผล\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'].values,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': y_test_station,\n",
    "        'Predicted_SPI': spi_predictions_2023\n",
    "    })\n",
    "    predicted_spi_results_DecisionTree = pd.concat([predicted_spi_results_DecisionTree, station_results], ignore_index=True)\n",
    "\n",
    "    # เพิ่มการคำนวณค่าเฉลี่ย SPI รายไตรมาส\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['Predicted_SPI'] = spi_predictions_2023\n",
    "    quarterly_spi = test_station.groupby('Quarter').agg({\n",
    "        'SPI': 'mean',\n",
    "        'Predicted_SPI': 'mean'\n",
    "    }).rename(columns={'SPI': 'Actual_SPI', 'Predicted_SPI': 'Predicted_SPI'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์ไตรมาสลงใน DataFrame\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': quarterly_spi.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': quarterly_spi['Actual_SPI'],\n",
    "        'Predicted_SPI': quarterly_spi['Predicted_SPI']\n",
    "    })\n",
    "    predicted_Quarterly_spi_results = pd.concat([predicted_Quarterly_spi_results, quarterly_results], ignore_index=True)\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลงใน CSV\n",
    "predicted_spi_results_DecisionTree.to_csv('predicted_spi_results_DecisionTree.csv', index=False, encoding='utf-8')\n",
    "print(\"Monthly prediction results have been saved to 'predicted_spi_results_DecisionTree.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลงใน CSV\n",
    "predicted_Quarterly_spi_results.to_csv('predicted_Quarterly_spi_Decisiontree.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly prediction results have been saved to 'predicted_Quarterly_spi_Decisiontree.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "xVNSs4m6TxCh",
    "outputId": "88e79142-cfc7-4cae-96a4-b021e909aef8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spei_results\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_data(last_date, features):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "    for feature in features:\n",
    "        dummy_data[feature] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)[feature].mean())\n",
    "    return dummy_data\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['PET', 'W']]\n",
    "    y_train_station = train_station['SPEI']\n",
    "\n",
    "    # สร้างและฝึกโมเดล Decision Tree\n",
    "    model = DecisionTreeRegressor(random_state=42)\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dummy_2024 = generate_2024_data(last_date, ['PET', 'W'])\n",
    "    dummy_2024['STATION'] = station\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2024\n",
    "    X_pred_2024 = dummy_2024[['PET', 'W']]\n",
    "    DecisionTree_Gxboost_spei_2024 = model.predict(X_pred_2024)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dummy_2024['DATETIME'], DecisionTree_Gxboost_spei_2024, label='Predicted SPEI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Predicted SPEI for 2024 (Decision Tree)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPEI for 2024 at station {station} (Decision Tree):\")\n",
    "    for date, prediction in zip(dummy_2024['DATETIME'], DecisionTree_Gxboost_spei_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    dummy_2024['DecisionTree_Predicted_SPEI'] = DecisionTree_Gxboost_spei_2024\n",
    "    DecisionTree_Quarterly_spei_2024 = dummy_2024.groupby('Quarter')['DecisionTree_Predicted_SPEI'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    DecisionTree_Quarterly_spei_2024.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPEI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPEI for 2024 (Decision Tree)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPEI for 2024 at station {station} (Decision Tree):\")\n",
    "    for quarter, value in DecisionTree_Quarterly_spei_2024.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n",
    "\n",
    "    # แสดงความสำคัญของ features\n",
    "    feature_importance = model.feature_importances_\n",
    "    features = ['PET', 'W']\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(features, feature_importance)\n",
    "    plt.title(f'Feature Importance for SPEI Prediction at station {station}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nFeature Importance for SPEI Prediction at station {station}:\")\n",
    "    for feature, importance in zip(features, feature_importance):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l3s9zeaVEXS"
   },
   "source": [
    "## Random Forest SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XIcZ-cV6WGJn",
    "outputId": "1515e890-fb31-4d3f-f0b4-f74dbdd2829f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spei_results[all_spei_results['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spei_results[all_spei_results['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือนและรายไตรมาส\n",
    "predicted_spei_results_RandomForest = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "predicted_quarterly_spei_results_RandomForest = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPEI_Quarterly', 'Predicted_SPEI_Quarterly'])\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['PET', 'W']]\n",
    "    y_train_station = train_station['SPEI']\n",
    "    X_test_station = test_station[['PET', 'W']]\n",
    "    y_test_station = test_station['SPEI']\n",
    "\n",
    "    # สร้างและฝึกโมเดล Random Forest\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    RandomForest_spei_2023 = model.predict(X_test_station)\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, RandomForest_spei_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'],\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI': y_test_station,\n",
    "        'Predicted_SPEI': RandomForest_spei_2023\n",
    "    })\n",
    "    predicted_spei_results_RandomForest = pd.concat([predicted_spei_results_RandomForest, station_results], ignore_index=True)\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPEI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], RandomForest_spei_2023, label='Predicted SPEI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPEI for 2023 (Random Forest)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPEI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], RandomForest_spei_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['RandomForest_Predicted_SPEI'] = RandomForest_spei_2023\n",
    "    RandomForest_Quarterly_spei_2023 = test_station.groupby('Quarter').agg({\n",
    "        'SPEI': 'mean',\n",
    "        'RandomForest_Predicted_SPEI': 'mean'\n",
    "    }).rename(columns={'SPEI': 'Actual', 'RandomForest_Predicted_SPEI': 'Predicted'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์ไตรมาสลงใน DataFrame รายไตรมาส\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': RandomForest_Quarterly_spei_2023.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI_Quarterly': RandomForest_Quarterly_spei_2023['Actual'],\n",
    "        'Predicted_SPEI_Quarterly': RandomForest_Quarterly_spei_2023['Predicted']\n",
    "    })\n",
    "    predicted_quarterly_spei_results_RandomForest = pd.concat([predicted_quarterly_spei_results_RandomForest, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    RandomForest_Quarterly_spei_2023[['Actual', 'Predicted']].plot(kind='bar', position=1, width=0.4, label=['Actual SPEI', 'Predicted SPEI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPEI for 2023 (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPEI for 2023 at station {station}:\")\n",
    "    for quarter, row in RandomForest_Quarterly_spei_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual: {row['Actual']:.4f}, Predicted: {row['Predicted']:.4f}\")\n",
    "\n",
    "    # แสดงความสำคัญของ features\n",
    "    feature_importance = model.feature_importances_\n",
    "    features = ['PET', 'W']\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(features, feature_importance)\n",
    "    plt.title(f'Feature Importance for SPEI Prediction at station {station}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nFeature Importance for SPEI Prediction at station {station}:\")\n",
    "    for feature, importance in zip(features, feature_importance):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spei_results_RandomForest.to_csv('predicted_spei_results_RandomForest.csv', index=False, encoding='utf-8')\n",
    "print(\"Results have been saved to 'predicted_spei_results_RandomForest.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_quarterly_spei_results_RandomForest.to_csv('predicted_Quarterly_spei_RandomForest.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly results have been saved to 'predicted_Quarterly_spei_RandomForest.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ezd41mFXbSo"
   },
   "outputs": [],
   "source": [
    "# ต้องแก้ไขเนื่องจากเอาโมเดล trree มารัน\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "# all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# # ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "# train_data = all_spei_results\n",
    "\n",
    "# # จัดการข้อมูลที่หายไป\n",
    "# train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# # รับรายการสถานี\n",
    "# stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# # ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "# def clean_data(df):\n",
    "#     mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "#     return df[mask]\n",
    "\n",
    "# # ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "# def generate_2024_data(last_date, features):\n",
    "#     next_year = last_date.year + 1\n",
    "#     dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "#     dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "#     for feature in features:\n",
    "#         dummy_data[feature] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)[feature].mean())\n",
    "#     return dummy_data\n",
    "\n",
    "# for station in stations:\n",
    "#     # กรองข้อมูลตามสถานี\n",
    "#     train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "#     # ทำความสะอาดข้อมูล\n",
    "#     train_station = clean_data(train_station)\n",
    "\n",
    "#     # สร้าง features และ target\n",
    "#     X_train_station = train_station[['PET', 'W']]\n",
    "#     y_train_station = train_station['SPEI']\n",
    "\n",
    "#     # สร้างโมเดล Decision Tree\n",
    "#     model = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "#     # ฝึกโมเดล\n",
    "#     model.fit(X_train_station, y_train_station)\n",
    "\n",
    "#     # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "#     last_date = train_station['DATETIME'].max()\n",
    "#     dummy_2024 = generate_2024_data(last_date, ['PET', 'W'])\n",
    "#     dummy_2024['STATION'] = station\n",
    "\n",
    "#     # ทำนายผลสำหรับปี 2024\n",
    "#     X_pred_2024 = dummy_2024[['PET', 'W']]\n",
    "#     DecisionTree_spei_2024 = model.predict(X_pred_2024)\n",
    "\n",
    "#     # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(dummy_2024['DATETIME'], DecisionTree_spei_2024, label='Predicted SPEI 2024', marker='x')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPEI')\n",
    "#     plt.title(f'Station: {station} - Predicted SPEI for 2024')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "#     print(f\"\\nPredicted SPEI for 2024 at station {station}:\")\n",
    "#     for date, prediction in zip(dummy_2024['DATETIME'], DecisionTree_spei_2024):\n",
    "#         print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "#     # คำนวณค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "#     dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "#     dummy_2024['DecisionTree_Predicted_SPEI'] = DecisionTree_spei_2024\n",
    "#     DecisionTree_Quarterly_spei_2024 = dummy_2024.groupby('Quarter')['DecisionTree_Predicted_SPEI'].mean()\n",
    "\n",
    "#     # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     DecisionTree_Quarterly_spei_2024.plot(kind='bar')\n",
    "#     plt.legend(['Predicted SPEI'])\n",
    "#     plt.xlabel('Quarter')\n",
    "#     plt.ylabel('Average SPEI')\n",
    "#     plt.title(f'Station: {station} - Quarterly Average Predicted SPEI for 2024')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # แสดงค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "#     print(f\"\\nQuarterly Average Predicted SPEI for 2024 at station {station}:\")\n",
    "#     for quarter, value in DecisionTree_Quarterly_spei_2024.items():\n",
    "#         print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj0bnWhlVMl_"
   },
   "source": [
    "##Random Forest SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WFpoI79lZLmx",
    "outputId": "6824eb12-533e-4c21-c445-9a6735e84d73"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# แยกข้อมูลสำหรับการฝึกและทดสอบ\n",
    "train_data = all_spi[all_spi['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spi[all_spi['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame สำหรับเก็บผลลัพธ์การทำนาย\n",
    "predicted_spi_results_RandomForest = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "# สร้าง DataFrame สำหรับเก็บผลลัพธ์รายไตรมาส\n",
    "predicted_quarterly_spi_results_RandomForest = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['SPI']]\n",
    "    y_train_station = train_station['SPI']\n",
    "    X_test_station = test_station[['SPI']]\n",
    "    y_test_station = test_station['SPI']\n",
    "\n",
    "    # สร้างและฝึกโมเดล Random Forest\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    RandomForest_spi_2023 = model.predict(X_test_station)\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, RandomForest_spi_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for SPI prediction in 2023: {mse}\")\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], RandomForest_spi_2023, label='Predicted SPI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPI for 2023')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], RandomForest_spi_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame สำหรับบันทึกผลรายเดือน\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'],\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': y_test_station,\n",
    "        'Predicted_SPI': RandomForest_spi_2023\n",
    "    })\n",
    "    predicted_spi_results_RandomForest = pd.concat([predicted_spi_results_RandomForest, station_results], ignore_index=True)\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPI รายไตรมาส\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['RandomForest_Predicted_SPI'] = RandomForest_spi_2023\n",
    "    quarterly_data = test_station.groupby('Quarter').agg({\n",
    "        'SPI': 'mean',  # Actual SPI\n",
    "        'RandomForest_Predicted_SPI': 'mean'  # Predicted SPI\n",
    "    }).rename(columns={'SPI': 'Actual_SPI', 'RandomForest_Predicted_SPI': 'Predicted_SPI'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายรายไตรมาสลงใน DataFrame สำหรับบันทึกผล\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': quarterly_data.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': quarterly_data['Actual_SPI'],\n",
    "        'Predicted_SPI': quarterly_data['Predicted_SPI']\n",
    "    })\n",
    "    predicted_quarterly_spi_results_RandomForest = pd.concat([predicted_quarterly_spi_results_RandomForest, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPI for 2023 at station {station}:\")\n",
    "    for quarter, row in quarterly_data.iterrows():\n",
    "        print(f\"{quarter}: Actual SPI: {row['Actual_SPI']:.4f}, Predicted SPI: {row['Predicted_SPI']:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลงใน CSV\n",
    "predicted_spi_results_RandomForest.to_csv('predicted_spi_results_RandomForest.csv', index=False, encoding='utf-8')\n",
    "print(\"Monthly prediction results have been saved to 'predicted_spi_results_RandomForest.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลงใน CSV\n",
    "predicted_quarterly_spi_results_RandomForest.to_csv('predicted_Quarterly_spi_RandomForest.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly prediction results have been saved to 'predicted_Quarterly_spi_RandomForest.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jou9ReUqZiaA",
    "outputId": "59263bf8-5f65-49dd-e93f-0270f4659448"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# สมมติว่าคุณได้โหลดข้อมูล SPI เข้ามาแล้วในตัวแปร all_spi\n",
    "# all_spi = pd.read_csv('path_to_spi_data.csv')\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spi\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_data(last_date):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "    dummy_data['SPI'] = dummy_data['DATETIME'].dt.month.map(train_data.groupby(train_data['DATETIME'].dt.month)['SPI'].mean())\n",
    "    return dummy_data\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # สร้าง features และ target\n",
    "    X_train_station = train_station[['SPI']]\n",
    "    y_train_station = train_station['SPI']\n",
    "\n",
    "    # สร้างโมเดล Random Forest Regressor\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=4)\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model.fit(X_train_station, y_train_station)\n",
    "\n",
    "    # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dummy_2024 = generate_2024_data(last_date)\n",
    "    dummy_2024['STATION'] = station\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2024\n",
    "    X_pred_2024 = dummy_2024[['SPI']]\n",
    "    RandomForest_spi_2024 = model.predict(X_pred_2024)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dummy_2024['DATETIME'], RandomForest_spi_2024, label='Predicted SPI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Predicted SPI for 2024')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPI for 2024 at station {station}:\")\n",
    "    for date, prediction in zip(dummy_2024['DATETIME'], RandomForest_spi_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    dummy_2024['Predicted_SPI'] = RandomForest_spi_2024\n",
    "    RandomForest_spi_2024_quarterly_data = dummy_2024.groupby('Quarter')['Predicted_SPI'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    RandomForest_spi_2024_quarterly_data.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPI for 2024')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPI for 2024 at station {station}:\")\n",
    "    for quarter, value in RandomForest_spi_2024_quarterly_data.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0s26l5p3EXx"
   },
   "source": [
    "## ARIMA SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LsL-9EhD3I8c",
    "outputId": "fb66be2c-0752-4d86-a795-2d5604d7fb46"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spei_results[all_spei_results['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spei_results[all_spei_results['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือนและรายไตรมาส\n",
    "predicted_spei_results_ARIMA = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "predicted_quarterly_spei_results_ARIMA = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPEI_Quarterly', 'Predicted_SPEI_Quarterly'])\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Processing station: {station}\")\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง Time Series ของ SPEI สำหรับการฝึกโมเดล\n",
    "    y_train_station = train_station.set_index('DATETIME')['SPEI']\n",
    "    y_test_station = test_station.set_index('DATETIME')['SPEI']\n",
    "\n",
    "    # ใช้ auto_arima เพื่อหาพารามิเตอร์ที่ดีที่สุด\n",
    "    try:\n",
    "        model = auto_arima(y_train_station, seasonal=False, stepwise=True, trace=True)\n",
    "        model_fitted = model.fit(y_train_station)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training ARIMA model for station {station}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    try:\n",
    "        ARIMA_spei_2023 = model_fitted.predict(n_periods=len(y_test_station))\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for station {station}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, ARIMA_spei_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': y_test_station.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI': y_test_station.values,\n",
    "        'Predicted_SPEI': ARIMA_spei_2023\n",
    "    })\n",
    "    predicted_spei_results_ARIMA = pd.concat([predicted_spei_results_ARIMA, station_results], ignore_index=True)\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_station.index, y_test_station, label='Actual SPEI 2023', marker='o')\n",
    "    plt.plot(y_test_station.index, ARIMA_spei_2023, label='Predicted SPEI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPEI for 2023 (ARIMA)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPEI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(y_test_station.index, ARIMA_spei_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    test_station_df = pd.DataFrame({'DATETIME': y_test_station.index, 'SPEI': y_test_station.values, 'ARIMA_Predicted_SPEI': ARIMA_spei_2023})\n",
    "    test_station_df['Quarter'] = test_station_df['DATETIME'].dt.to_period('Q')\n",
    "    ARIMA_Quarterly_spei_2023 = test_station_df.groupby('Quarter').agg({\n",
    "        'SPEI': 'mean',\n",
    "        'ARIMA_Predicted_SPEI': 'mean'\n",
    "    }).rename(columns={'SPEI': 'Actual', 'ARIMA_Predicted_SPEI': 'Predicted'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์ไตรมาสลงใน DataFrame รายไตรมาส\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': ARIMA_Quarterly_spei_2023.index.astype(str),\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI_Quarterly': ARIMA_Quarterly_spei_2023['Actual'],\n",
    "        'Predicted_SPEI_Quarterly': ARIMA_Quarterly_spei_2023['Predicted']\n",
    "    })\n",
    "    predicted_quarterly_spei_results_ARIMA = pd.concat([predicted_quarterly_spei_results_ARIMA, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ARIMA_Quarterly_spei_2023[['Actual', 'Predicted']].plot(kind='bar', position=1, width=0.4, label=['Actual SPEI', 'Predicted SPEI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPEI for 2023 (ARIMA)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPEI for 2023 at station {station}:\")\n",
    "    for quarter, row in ARIMA_Quarterly_spei_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual: {row['Actual']:.4f}, Predicted: {row['Predicted']:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spei_results_ARIMA.to_csv('predicted_spei_results_ARIMA.csv', index=False, encoding='utf-8')\n",
    "print(\"Results have been saved to 'predicted_spei_results_ARIMA.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_quarterly_spei_results_ARIMA.to_csv('predicted_Quarterly_spei_ARIMA.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly results have been saved to 'predicted_Quarterly_spei_ARIMA.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sCfeEc_V30Je",
    "outputId": "287e7332-98f6-4296-9e55-e203cb2e1021"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "train_data = all_spei_results\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "def generate_2024_dates(last_date):\n",
    "    next_year = last_date.year + 1\n",
    "    dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "    return dates\n",
    "\n",
    "for station in stations:\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลเพียงพอสำหรับการฝึกโมเดล ARIMA หรือไม่\n",
    "    if len(train_station) < 20:  # ต้องมีข้อมูลมากกว่า 20 จุดข้อมูล\n",
    "        print(f\"Not enough data to fit ARIMA model for station {station}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง target คือคอลัมน์ SPEI\n",
    "    y_train_station = train_station['SPEI'].values\n",
    "\n",
    "    # ฝึกโมเดล ARIMA (ใช้ค่า p, d, q = (1, 1, 1) เป็นค่าเริ่มต้น สามารถปรับเปลี่ยนได้ตามความเหมาะสม)\n",
    "    arima_model = ARIMA(y_train_station, order=(1, 1, 1))\n",
    "    arima_result = arima_model.fit()\n",
    "\n",
    "    # สร้างข้อมูลจำลองวันที่สำหรับปี 2024\n",
    "    last_date = train_station['DATETIME'].max()\n",
    "    dates_2024 = generate_2024_dates(last_date)\n",
    "\n",
    "    # ทำนายค่า SPEI สำหรับปี 2024 (12 เดือนข้างหน้า)\n",
    "    ARIMA_spei_2024 = arima_result.forecast(steps=12)\n",
    "\n",
    "    # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates_2024, ARIMA_spei_2024, label='Predicted SPEI 2024', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Predicted SPEI for 2024')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "    print(f\"\\nPredicted SPEI for 2024 at station {station}:\")\n",
    "    for date, prediction in zip(dates_2024, ARIMA_spei_2024):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    dummy_2024 = pd.DataFrame({'DATETIME': dates_2024, 'ARIMA_spei_2024': ARIMA_spei_2024})\n",
    "    dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "    ARIMA_spei_quarterly_2024 = dummy_2024.groupby('Quarter')['ARIMA_spei_2024'].mean()\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ARIMA_spei_quarterly_2024.plot(kind='bar')\n",
    "    plt.legend(['Predicted SPEI'])\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average Predicted SPEI for 2024')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาสสำหรับปี 2024\n",
    "    print(f\"\\nQuarterly Average Predicted SPEI for 2024 at station {station}:\")\n",
    "    for quarter, value in ARIMA_spei_quarterly_2024.items():\n",
    "        print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8ZVa8Wt321s"
   },
   "source": [
    "##ARIMA SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nhfWVTuW37O4",
    "outputId": "8b58dbb2-2503-4e32-9bd7-438d7119fc7b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spi[all_spi['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spi[all_spi['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือนและรายไตรมาส\n",
    "predicted_spi_results_ARIMA = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "predicted_quarterly_spi_results_ARIMA = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPI_Quarterly', 'Predicted_SPI_Quarterly'])\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Processing station: {station}\")\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง Time Series ของ SPI สำหรับการฝึกโมเดล\n",
    "    y_train_station = train_station.set_index('DATETIME')['SPI']\n",
    "    y_test_station = test_station.set_index('DATETIME')['SPI']\n",
    "\n",
    "    # ใช้ auto_arima เพื่อหาพารามิเตอร์ที่ดีที่สุด\n",
    "    try:\n",
    "        model = auto_arima(y_train_station, seasonal=False, stepwise=True, trace=True)\n",
    "        model_fitted = model.fit(y_train_station)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training ARIMA model for station {station}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    try:\n",
    "        ARIMA_spi_2023 = model_fitted.predict(n_periods=len(y_test_station))\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for station {station}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, ARIMA_spi_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': y_test_station.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': y_test_station.values,\n",
    "        'Predicted_SPI': ARIMA_spi_2023\n",
    "    })\n",
    "    predicted_spi_results_ARIMA = pd.concat([predicted_spi_results_ARIMA, station_results], ignore_index=True)\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_station.index, y_test_station, label='Actual SPI 2023', marker='o')\n",
    "    plt.plot(y_test_station.index, ARIMA_spi_2023, label='Predicted SPI 2023', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPI for 2023 (ARIMA)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(y_test_station.index, ARIMA_spi_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPI รายไตรมาส\n",
    "    test_station_df = pd.DataFrame({'DATETIME': y_test_station.index, 'SPI': y_test_station.values, 'ARIMA_Predicted_SPI': ARIMA_spi_2023})\n",
    "    test_station_df['Quarter'] = test_station_df['DATETIME'].dt.to_period('Q')\n",
    "    ARIMA_Quarterly_spi_2023 = test_station_df.groupby('Quarter').agg({\n",
    "        'SPI': 'mean',\n",
    "        'ARIMA_Predicted_SPI': 'mean'\n",
    "    }).rename(columns={'SPI': 'Actual', 'ARIMA_Predicted_SPI': 'Predicted'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์ไตรมาสลงใน DataFrame รายไตรมาส\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': ARIMA_Quarterly_spi_2023.index.astype(str),\n",
    "        'STATION': station,\n",
    "        'Actual_SPI_Quarterly': ARIMA_Quarterly_spi_2023['Actual'],\n",
    "        'Predicted_SPI_Quarterly': ARIMA_Quarterly_spi_2023['Predicted']\n",
    "    })\n",
    "    predicted_quarterly_spi_results_ARIMA = pd.concat([predicted_quarterly_spi_results_ARIMA, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ARIMA_Quarterly_spi_2023[['Actual', 'Predicted']].plot(kind='bar', position=1, width=0.4, label=['Actual SPI', 'Predicted SPI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPI for 2023 (ARIMA)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPI for 2023 at station {station}:\")\n",
    "    for quarter, row in ARIMA_Quarterly_spi_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual: {row['Actual']:.4f}, Predicted: {row['Predicted']:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spi_results_ARIMA.to_csv('predicted_spi_results_ARIMA.csv', index=False, encoding='utf-8')\n",
    "print(\"Results have been saved to 'predicted_spi_results_ARIMA.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_quarterly_spi_results_ARIMA.to_csv('predicted_Quarterly_spi_ARIMA.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly results have been saved to 'predicted_Quarterly_spi_ARIMA.csv'\")\n",
    "\n",
    "# แสดงตัวอย่างข้อมูล\n",
    "print(predicted_spi_results_ARIMA.head(20))\n",
    "print(predicted_quarterly_spi_results_ARIMA.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTMb3RoE4bCZ"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')  # ปิดการแจ้งเตือนที่ไม่จำเป็น\n",
    "\n",
    "# # แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "# all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# # ใช้ข้อมูลทั้งหมดเป็นชุด training\n",
    "# train_data = all_spi\n",
    "\n",
    "# # จัดการข้อมูลที่หายไป\n",
    "# train_data = train_data.fillna(method='ffill')\n",
    "\n",
    "# # รับรายการสถานี\n",
    "# stations = all_spi['STATION'].unique()\n",
    "\n",
    "# # ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "# def clean_data(df):\n",
    "#     mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "#     return df[mask]\n",
    "\n",
    "# # ฟังก์ชันสำหรับสร้างข้อมูลจำลองสำหรับปี 2024\n",
    "# def generate_2024_data(last_date):\n",
    "#     next_year = last_date.year + 1\n",
    "#     dates = pd.date_range(start=f'{next_year}-01-01', end=f'{next_year}-12-31', freq='MS')\n",
    "#     dummy_data = pd.DataFrame({'DATETIME': dates})\n",
    "#     return dummy_data\n",
    "\n",
    "# for station in stations:\n",
    "#     # กรองข้อมูลตามสถานี\n",
    "#     train_station = train_data[train_data['STATION'] == station]\n",
    "\n",
    "#     # ทำความสะอาดข้อมูล\n",
    "#     train_station = clean_data(train_station)\n",
    "\n",
    "#     # สร้าง ARIMA โมเดล\n",
    "#     # ใช้ข้อมูล SPI เป็นข้อมูล Time Series\n",
    "#     spi_series = train_station.set_index('DATETIME')['SPI']\n",
    "\n",
    "#     # สร้างโมเดล ARIMA โดยกำหนดค่า p, d, q (ปรับตามความเหมาะสม)\n",
    "#     model = ARIMA(spi_series, order=(5, 1, 0))  # ตัวอย่างใช้ p=5, d=1, q=0\n",
    "#     model_fit = model.fit()\n",
    "\n",
    "#     # สร้างข้อมูลจำลองสำหรับปี 2024\n",
    "#     last_date = train_station['DATETIME'].max()\n",
    "#     dummy_2024 = generate_2024_data(last_date)\n",
    "#     dummy_2024['STATION'] = station\n",
    "\n",
    "#     # ทำนายค่า SPI สำหรับปี 2024\n",
    "#     ARIMA_spi_2024 = model_fit.forecast(steps=12)  # ทำนาย 12 เดือน\n",
    "\n",
    "#     # แสดงผลการพยากรณ์สำหรับปี 2024\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(dummy_2024['DATETIME'], ARIMA_spi_2024, label='Predicted SPI 2024', marker='x')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPI')\n",
    "#     plt.title(f'Station: {station} - Predicted SPI for 2024')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # แสดงค่าพยากรณ์สำหรับปี 2024\n",
    "#     print(f\"\\nPredicted SPI for 2024 at station {station}:\")\n",
    "#     for date, prediction in zip(dummy_2024['DATETIME'], ARIMA_spi_2024):\n",
    "#         print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}\")\n",
    "\n",
    "#     # คำนวณค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "#     dummy_2024['Quarter'] = dummy_2024['DATETIME'].dt.to_period('Q')\n",
    "#     dummy_2024['ARIMA_spi_2024'] = ARIMA_spi_2024\n",
    "#     ARIMA_spi_quarterly_2024 = dummy_2024.groupby('Quarter')['ARIMA_spi_2024'].mean()\n",
    "\n",
    "#     # แสดงกราฟค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     ARIMA_spi_quarterly_2024.plot(kind='bar')\n",
    "#     plt.legend(['Predicted SPI'])\n",
    "#     plt.xlabel('Quarter')\n",
    "#     plt.ylabel('Average SPI')\n",
    "#     plt.title(f'Station: {station} - Quarterly Average Predicted SPI for 2024')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # แสดงค่าเฉลี่ย SPI รายไตรมาสสำหรับปี 2024\n",
    "#     print(f\"\\nQuarterly Average Predicted SPI for 2024 at station {station}:\")\n",
    "#     for quarter, value in ARIMA_spi_quarterly_2024.items():\n",
    "#         print(f\"{quarter}: Predicted: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLbUNXlfVUxw"
   },
   "source": [
    "## LSTM SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b5_Kgwd2V-o4",
    "outputId": "1b7b953e-c08b-4fa8-8068-9d12dc642586"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# แปลงคอลัมน์ 'DATETIME' เป็น datetime object\n",
    "all_spei_results['DATETIME'] = pd.to_datetime(all_spei_results['DATETIME'])\n",
    "\n",
    "# แยกชุดฝึกและชุดทดสอบตามปี\n",
    "train_data = all_spei_results[all_spei_results['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spei_results[all_spei_results['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# จัดการข้อมูลที่หายไป\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# รับรายการสถานี\n",
    "stations = all_spei_results['STATION'].unique()\n",
    "\n",
    "# ฟังก์ชันสำหรับการทำความสะอาดข้อมูล\n",
    "def clean_data(df):\n",
    "    mask = (df['SPEI'].notna()) & (df['SPEI'] != float('inf')) & (df['SPEI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# ฟังก์ชันสร้างลำดับของข้อมูลสำหรับการป้อนเข้า LSTM\n",
    "def create_sequences(data, time_steps=1):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        x.append(data[i:(i + time_steps)])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# สร้าง DataFrame เก็บผลลัพธ์การทำนายรายเดือนและรายไตรมาส\n",
    "predicted_spei_results_LSTM = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPEI', 'Predicted_SPEI'])\n",
    "predicted_quarterly_spei_results_LSTM = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPEI_Quarterly', 'Predicted_SPEI_Quarterly'])\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Processing station: {station}\")\n",
    "\n",
    "    # กรองข้อมูลตามสถานี\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # ทำความสะอาดข้อมูล\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # ตรวจสอบว่ามีข้อมูลสำหรับปี 2023 หรือไม่\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # สร้าง Time Series ของ SPEI สำหรับการฝึกโมเดล\n",
    "    y_train_station = train_station.set_index('DATETIME')['SPEI'].values\n",
    "    y_test_station = test_station.set_index('DATETIME')['SPEI'].values\n",
    "\n",
    "    # สเกลข้อมูลให้อยู่ในช่วง [0,1] เพื่อใช้ใน LSTM\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_train_station_scaled = scaler.fit_transform(y_train_station.reshape(-1, 1))\n",
    "\n",
    "    # กำหนดจำนวน time steps (ลำดับเวลา) สำหรับ LSTM\n",
    "    time_steps = 12\n",
    "    X_train, y_train = create_sequences(y_train_station_scaled, time_steps)\n",
    "\n",
    "    # ปรับรูปทรงข้อมูลให้เข้ากับ LSTM (ต้องเป็นรูปทรง [samples, time steps, features])\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # สร้างโมเดล LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # คอมไพล์โมเดล\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # ฝึกโมเดล\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "    # เตรียมข้อมูลสำหรับการพยากรณ์ในปี 2023\n",
    "    total_data = np.concatenate((y_train_station_scaled, scaler.transform(y_test_station.reshape(-1, 1))), axis=0)\n",
    "    test_inputs = total_data[len(total_data) - len(y_test_station) - time_steps:]\n",
    "    X_test, _ = create_sequences(test_inputs, time_steps)\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # ทำนายผลสำหรับปี 2023\n",
    "    LSTM_spei_2023 = model.predict(X_test)\n",
    "    LSTM_spei_2023 = scaler.inverse_transform(LSTM_spei_2023).flatten()\n",
    "\n",
    "    # ประเมินผล\n",
    "    mse = mean_squared_error(y_test_station, LSTM_spei_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # เพิ่มผลลัพธ์การทำนายลงใน DataFrame\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'].values,\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI': y_test_station,\n",
    "        'Predicted_SPEI': LSTM_spei_2023\n",
    "    })\n",
    "    predicted_spei_results_LSTM = pd.concat([predicted_spei_results_LSTM, station_results], ignore_index=True)\n",
    "\n",
    "    # แสดงผลการพยากรณ์เทียบกับค่าจริงปี 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPEI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], LSTM_spei_2023, label='Predicted SPEI 2023 (LSTM)', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPEI for 2023 (LSTM)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าพยากรณ์และค่าจริงสำหรับปี 2023\n",
    "    print(f\"\\nPredicted vs Actual SPEI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], LSTM_spei_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # คำนวณค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    test_station_df = pd.DataFrame({'DATETIME': test_station['DATETIME'], 'SPEI': y_test_station, 'LSTM_Predicted_SPEI': LSTM_spei_2023})\n",
    "    test_station_df['Quarter'] = test_station_df['DATETIME'].dt.to_period('Q')\n",
    "    LSTM_Quarterly_spei_2023 = test_station_df.groupby('Quarter').agg({\n",
    "        'SPEI': 'mean',\n",
    "        'LSTM_Predicted_SPEI': 'mean'\n",
    "    }).rename(columns={'SPEI': 'Actual', 'LSTM_Predicted_SPEI': 'Predicted'})\n",
    "\n",
    "    # เพิ่มผลลัพธ์ไตรมาสลงใน DataFrame รายไตรมาส\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': LSTM_Quarterly_spei_2023.index.astype(str),\n",
    "        'STATION': station,\n",
    "        'Actual_SPEI_Quarterly': LSTM_Quarterly_spei_2023['Actual'],\n",
    "        'Predicted_SPEI_Quarterly': LSTM_Quarterly_spei_2023['Predicted']\n",
    "    })\n",
    "    predicted_quarterly_spei_results_LSTM = pd.concat([predicted_quarterly_spei_results_LSTM, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # แสดงกราฟค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    LSTM_Quarterly_spei_2023[['Actual', 'Predicted']].plot(kind='bar', position=1, width=0.4, label=['Actual SPEI', 'Predicted SPEI'])\n",
    "    plt.legend()\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel('Average SPEI')\n",
    "    plt.title(f'Station: {station} - Quarterly Average SPEI for 2023 (LSTM)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # แสดงค่าเฉลี่ย SPEI รายไตรมาส\n",
    "    print(f\"\\nQuarterly Average SPEI for 2023 at station {station}:\")\n",
    "    for quarter, row in LSTM_Quarterly_spei_2023.iterrows():\n",
    "        print(f\"{quarter}: Actual: {row['Actual']:.4f}, Predicted: {row['Predicted']:.4f}\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายเดือนลง CSV\n",
    "predicted_spei_results_LSTM.to_csv('predicted_spei_results_LSTM.csv', index=False, encoding='utf-8')\n",
    "print(\"Results have been saved to 'predicted_spei_results_LSTM.csv'\")\n",
    "\n",
    "# บันทึกผลลัพธ์รายไตรมาสลง CSV\n",
    "predicted_quarterly_spei_results_LSTM.to_csv('predicted_Quarterly_spei_LSTM.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly results have been saved to 'predicted_Quarterly_spei_LSTM.csv'\")\n",
    "\n",
    "# แสดงตัวอย่างข้อมูล\n",
    "print(predicted_spei_results_LSTM.head(20))\n",
    "print(predicted_quarterly_spei_results_LSTM.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osHMDqROVavR"
   },
   "source": [
    "##LSTM SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mC-OIPgSiFj0",
    "outputId": "942b6984-948d-4d27-c8d5-1fb557f8c879"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the SPI data\n",
    "all_spi['DATETIME'] = pd.to_datetime(all_spi['DATETIME'])\n",
    "\n",
    "# Separate training data (up to 2022) and test data (for 2023)\n",
    "train_data = all_spi[all_spi['DATETIME'].dt.year <= 2022]\n",
    "test_data = all_spi[all_spi['DATETIME'].dt.year == 2023]\n",
    "\n",
    "# Handle missing values\n",
    "train_data = train_data.fillna(method='ffill')\n",
    "test_data = test_data.fillna(method='ffill')\n",
    "\n",
    "# Get the list of unique stations\n",
    "stations = all_spi['STATION'].unique()\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(df):\n",
    "    mask = (df['SPI'].notna()) & (df['SPI'] != float('inf')) & (df['SPI'] != -float('inf'))\n",
    "    return df[mask]\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, time_steps=1):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        x.append(data[i:(i + time_steps)])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# DataFrame to store monthly prediction results\n",
    "predicted_spi_results_LSTM = pd.DataFrame(columns=['DATETIME', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "# DataFrame to store quarterly prediction results\n",
    "predicted_quarterly_spi_results_LSTM = pd.DataFrame(columns=['Quarter', 'STATION', 'Actual_SPI', 'Predicted_SPI'])\n",
    "\n",
    "for station in stations:\n",
    "    print(f\"Processing station: {station}\")\n",
    "\n",
    "    # Filter data by station\n",
    "    train_station = train_data[train_data['STATION'] == station]\n",
    "    test_station = test_data[test_data['STATION'] == station]\n",
    "\n",
    "    # Clean data\n",
    "    train_station = clean_data(train_station)\n",
    "    test_station = clean_data(test_station)\n",
    "\n",
    "    # Check if there's test data for 2023\n",
    "    if test_station.empty:\n",
    "        print(f\"No data available for 2023 at station {station}. Skipping this station.\")\n",
    "        continue\n",
    "\n",
    "    # Time Series for SPI (training and testing)\n",
    "    y_train_station = train_station.set_index('DATETIME')['SPI'].values\n",
    "    y_test_station = test_station.set_index('DATETIME')['SPI'].values\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_train_station_scaled = scaler.fit_transform(y_train_station.reshape(-1, 1))\n",
    "\n",
    "    # Define the time step window for LSTM\n",
    "    time_steps = 12\n",
    "    X_train, y_train = create_sequences(y_train_station_scaled, time_steps)\n",
    "\n",
    "    # Reshape data for LSTM input\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "    # Prepare the test data for 2023 prediction\n",
    "    total_data = np.concatenate((y_train_station_scaled, scaler.transform(y_test_station.reshape(-1, 1))), axis=0)\n",
    "    test_inputs = total_data[len(total_data) - len(y_test_station) - time_steps:]\n",
    "    X_test, _ = create_sequences(test_inputs, time_steps)\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Predict SPI for 2023\n",
    "    LSTM_spi_2023 = model.predict(X_test)\n",
    "    LSTM_spi_2023 = scaler.inverse_transform(LSTM_spi_2023).flatten()\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test_station, LSTM_spi_2023)\n",
    "    print(f\"Station: {station}, Mean Squared Error for 2023: {mse}\")\n",
    "\n",
    "    # Store monthly prediction results in the DataFrame\n",
    "    station_results = pd.DataFrame({\n",
    "        'DATETIME': test_station['DATETIME'].values,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': y_test_station,\n",
    "        'Predicted_SPI': LSTM_spi_2023\n",
    "    })\n",
    "    predicted_spi_results_LSTM = pd.concat([predicted_spi_results_LSTM, station_results], ignore_index=True)\n",
    "\n",
    "    # Calculate quarterly average SPI\n",
    "    test_station['Quarter'] = test_station['DATETIME'].dt.to_period('Q')\n",
    "    test_station['LSTM_Predicted_SPI'] = LSTM_spi_2023\n",
    "    quarterly_data = test_station.groupby('Quarter').agg({\n",
    "        'SPI': 'mean',  # Actual SPI\n",
    "        'LSTM_Predicted_SPI': 'mean'  # Predicted SPI\n",
    "    }).rename(columns={'SPI': 'Actual_SPI', 'LSTM_Predicted_SPI': 'Predicted_SPI'})\n",
    "\n",
    "    # Store quarterly prediction results in the DataFrame\n",
    "    quarterly_results = pd.DataFrame({\n",
    "        'Quarter': quarterly_data.index,\n",
    "        'STATION': station,\n",
    "        'Actual_SPI': quarterly_data['Actual_SPI'],\n",
    "        'Predicted_SPI': quarterly_data['Predicted_SPI']\n",
    "    })\n",
    "    predicted_quarterly_spi_results_LSTM = pd.concat([predicted_quarterly_spi_results_LSTM, quarterly_results], ignore_index=True)\n",
    "\n",
    "    # Plot the monthly prediction vs actual for 2023\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_station['DATETIME'], y_test_station, label='Actual SPI 2023', marker='o')\n",
    "    plt.plot(test_station['DATETIME'], LSTM_spi_2023, label='Predicted SPI 2023 (LSTM)', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.title(f'Station: {station} - Actual vs Predicted SPI for 2023 (LSTM)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display monthly prediction results for 2023\n",
    "    print(f\"\\nPredicted vs Actual SPI for 2023 at station {station}:\")\n",
    "    for date, prediction, actual in zip(test_station['DATETIME'], LSTM_spi_2023, y_test_station):\n",
    "        print(f\"{date.strftime('%Y-%m')}: Predicted: {prediction:.4f}, Actual: {actual:.4f}\")\n",
    "\n",
    "    # Display quarterly average SPI\n",
    "    print(f\"\\nQuarterly Average SPI for 2023 at station {station}:\")\n",
    "    for quarter, row in quarterly_data.iterrows():\n",
    "        print(f\"{quarter}: Actual SPI: {row['Actual_SPI']:.4f}, Predicted SPI: {row['Predicted_SPI']:.4f}\")\n",
    "\n",
    "# Save the monthly prediction results to CSV\n",
    "predicted_spi_results_LSTM.to_csv('predicted_spi_results_LSTM.csv', index=False, encoding='utf-8')\n",
    "print(\"Monthly prediction results have been saved to 'predicted_spi_results_LSTM.csv'\")\n",
    "\n",
    "# Save the quarterly prediction results to CSV\n",
    "predicted_quarterly_spi_results_LSTM.to_csv('predicted_Quarterly_spi_LSTM.csv', index=False, encoding='utf-8')\n",
    "print(\"Quarterly prediction results have been saved to 'predicted_Quarterly_spi_LSTM.csv'\")\n",
    "\n",
    "# Display the first 20 rows of the monthly prediction results\n",
    "print(predicted_spi_results_LSTM.head(20))\n",
    "\n",
    "# Display the first 20 rows of the quarterly prediction results\n",
    "print(predicted_quarterly_spi_results_LSTM.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV_WX-_zaK8y"
   },
   "source": [
    "# Comparison of prediction results in each model Gxboost_DecisionTree_RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4YUPGmWaoxo"
   },
   "source": [
    "##Comparison SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4IU6YY_Hk8LC",
    "outputId": "b008741c-4272-4996-d416-ab0b4e9126fd"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from glob import glob\n",
    "\n",
    "# # อ่านข้อมูลจากไฟล์ CSV ทั้งหมด\n",
    "# file_paths = [\n",
    "#     '/content/predicted_spei_results_ARIMA.csv',\n",
    "#     '/content/predicted_spei_results_Decisiontree.csv',\n",
    "#     '/content/predicted_spei_results_RandomForest.csv',\n",
    "#     '/content/predicted_spei_results_Xgboost.csv',\n",
    "#     '/content/predicted_spei_results_LSTM.csv'\n",
    "# ]\n",
    "\n",
    "# dfs = []\n",
    "# for file_path in file_paths:\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     model_name = file_path.split('_')[-1].split('.')[0]\n",
    "#     df['Model'] = model_name\n",
    "#     dfs.append(df)\n",
    "\n",
    "# # รวมข้อมูลทั้งหมด\n",
    "# combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # แปลง DATETIME เป็น datetime object\n",
    "# combined_df['DATETIME'] = pd.to_datetime(combined_df['DATETIME'])\n",
    "\n",
    "# # ฟังก์ชันสำหรับสร้างกราฟแต่ละสถานี\n",
    "# def plot_station_comparison(station_data, station_name):\n",
    "#     plt.figure(figsize=(18, 12))\n",
    "#     plt.suptitle(f'Station: {station_name} - Comparison of Models', fontsize=16)\n",
    "\n",
    "#     # กราฟเส้นเปรียบเทียบค่าจริงกับค่าทำนาย\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         plt.plot(model_data['DATETIME'], model_data['Predicted_SPEI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "#     actual_data = station_data.drop_duplicates(subset=['DATETIME'])\n",
    "#     plt.plot(actual_data['DATETIME'], actual_data['Actual_SPEI'], label='Actual', linewidth=2)\n",
    "\n",
    "#     plt.title('Actual vs Predicted SPEI for 2023')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPEI')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบ MSE\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     mse_data = []\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         mse = ((model_data['Actual_SPEI'] - model_data['Predicted_SPEI'])**2).mean()\n",
    "#         mse_data.append({'Model': model, 'MSE': mse})\n",
    "\n",
    "#     mse_df = pd.DataFrame(mse_data)\n",
    "#     sns.barplot(x='Model', y='MSE', data=mse_df)\n",
    "#     plt.title('Mean Squared Error Comparison')\n",
    "#     plt.xlabel('Model')\n",
    "#     plt.ylabel('MSE')\n",
    "#     plt.xticks(rotation=45)\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบค่าเฉลี่ย SPEI รายไตรมาส\n",
    "#     plt.subplot(2, 2, 3)\n",
    "#     station_data['Quarter'] = station_data['DATETIME'].dt.to_period('Q')\n",
    "#     quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPEI'].mean().reset_index()\n",
    "#     actual_quarterly = station_data.groupby('Quarter')['Actual_SPEI'].mean().reset_index()\n",
    "#     actual_quarterly['Model'] = 'Actual'\n",
    "#     quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "#     sns.barplot(x='Quarter', y='Predicted_SPEI', hue='Model', data=quarterly_data)\n",
    "#     plt.title('Quarterly Average SPEI Comparison')\n",
    "#     plt.xlabel('Quarter')\n",
    "#     plt.ylabel('Average SPEI')\n",
    "#     plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # สร้างกราฟสำหรับแต่ละสถานี\n",
    "# for station in combined_df['STATION'].unique():\n",
    "#     station_data = combined_df[combined_df['STATION'] == station]\n",
    "#     plot_station_comparison(station_data, station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "emf4S6BHVdNQ",
    "outputId": "8a08549c-9ca3-46ac-e085-7d99389c52ea"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # ฟังก์ชันสำหรับอ่านข้อมูลและเตรียม DataFrame\n",
    "# def load_data(file_paths, frequency):\n",
    "#     dfs = []\n",
    "#     for file_path in file_paths:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # ดึงชื่อโมเดลจากชื่อไฟล์\n",
    "#         model_name = file_path.split('_')[-1].split('.')[0]\n",
    "#         df['Model'] = model_name\n",
    "#         df['Frequency'] = frequency\n",
    "#         # เปลี่ยนชื่อคอลัมน์เพื่อให้สอดคล้องกัน\n",
    "#         if frequency == 'Quarterly':\n",
    "#             df.rename(columns={\n",
    "#                 'Quarter': 'Time',\n",
    "#                 'Actual_SPEI_Quarterly': 'Actual_SPEI',\n",
    "#                 'Predicted_SPEI_Quarterly': 'Predicted_SPEI'\n",
    "#             }, inplace=True)\n",
    "#             # แปลง 'Time' ให้เป็น datetime โดยใช้ pandas.Period\n",
    "#             df['Time'] = df['Time'].apply(lambda x: pd.Period(x, freq='Q').start_time)\n",
    "#         else:\n",
    "#             df.rename(columns={'DATETIME': 'Time'}, inplace=True)\n",
    "#             df['Time'] = pd.to_datetime(df['Time'])\n",
    "#         dfs.append(df)\n",
    "#     combined = pd.concat(dfs, ignore_index=True)\n",
    "#     return combined\n",
    "\n",
    "# # เส้นทางไฟล์สำหรับข้อมูลรายเดือน\n",
    "# monthly_file_paths = [\n",
    "#     '/content/predicted_spei_results_ARIMA.csv',\n",
    "#     '/content/predicted_spei_results_Decisiontree.csv',\n",
    "#     '/content/predicted_spei_results_RandomForest.csv',\n",
    "#     '/content/predicted_spei_results_Xgboost.csv',\n",
    "#     '/content/predicted_spei_results_LSTM.csv'\n",
    "# ]\n",
    "\n",
    "# # เส้นทางไฟล์สำหรับข้อมูลไตรมาส\n",
    "# quarterly_file_paths = [\n",
    "#     '/content/predicted_Quarterly_spei_LSTM.csv',\n",
    "#     '/content/predicted_Quarterly_spei_ARIMA.csv',\n",
    "#     '/content/predicted_Quarterly_spei_RandomForest.csv',\n",
    "#     '/content/predicted_Quarterly_spei_xgboost.csv',\n",
    "#     '/content/predicted_Quarterly_spei_Decisiontree.csv'\n",
    "# ]\n",
    "\n",
    "# # โหลดข้อมูลรายเดือนและไตรมาส\n",
    "# monthly_df = load_data(monthly_file_paths, 'Monthly')\n",
    "# quarterly_df = load_data(quarterly_file_paths, 'Quarterly')\n",
    "\n",
    "# # รวมข้อมูลทั้งสองประเภทเข้าด้วยกัน\n",
    "# combined_df = pd.concat([monthly_df, quarterly_df], ignore_index=True)\n",
    "\n",
    "# # ฟังก์ชันสำหรับสร้างกราฟแต่ละสถานีและแต่ละความถี่\n",
    "# def plot_station_comparison(station_data, station_name, frequency):\n",
    "#     plt.figure(figsize=(18, 18))\n",
    "#     plt.suptitle(f'Station: {station_name} - Comparison of Models ({frequency})', fontsize=16)\n",
    "\n",
    "#     # กราฟเส้นเปรียบเทียบค่าจริงกับค่าทำนาย\n",
    "#     plt.subplot(3, 2, 1)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         plt.plot(model_data['Time'], model_data['Predicted_SPEI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "#     actual_data = station_data.drop_duplicates(subset=['Time'])\n",
    "#     plt.plot(actual_data['Time'], actual_data['Actual_SPEI'], label='Actual', linewidth=2)\n",
    "\n",
    "#     plt.title(f'Actual vs Predicted SPEI for {frequency}')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPEI')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบ MSE, MAE, RMSE, R²\n",
    "#     plt.subplot(3, 2, 2)\n",
    "#     error_data = []\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         mse = mean_squared_error(model_data['Actual_SPEI'], model_data['Predicted_SPEI'])\n",
    "#         mae = mean_absolute_error(model_data['Actual_SPEI'], model_data['Predicted_SPEI'])\n",
    "#         rmse = mean_squared_error(model_data['Actual_SPEI'], model_data['Predicted_SPEI'], squared=False)\n",
    "#         r2 = r2_score(model_data['Actual_SPEI'], model_data['Predicted_SPEI'])\n",
    "#         error_data.append({'Model': model, 'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "#     error_df = pd.DataFrame(error_data)\n",
    "#     error_melted = error_df.melt(id_vars='Model', value_vars=['MSE', 'MAE', 'RMSE', 'R2'], var_name='Metric', value_name='Value')\n",
    "\n",
    "#     sns.barplot(x='Model', y='Value', hue='Metric', data=error_melted)\n",
    "#     plt.title('Error Metrics Comparison')\n",
    "#     plt.xlabel('Model')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.legend(title='Metric', loc='upper right')\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบค่าเฉลี่ย SPEI รายไตรมาส (เฉพาะข้อมูลแบบรายเดือน)\n",
    "#     if frequency == 'Monthly':\n",
    "#         plt.subplot(3, 2, 3)\n",
    "#         station_data['Quarter'] = station_data['Time'].dt.to_period('Q')\n",
    "#         quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPEI'].mean().reset_index()\n",
    "#         actual_quarterly = station_data.groupby('Quarter')['Actual_SPEI'].mean().reset_index()\n",
    "#         actual_quarterly['Model'] = 'Actual'\n",
    "#         quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "#         sns.barplot(x='Quarter', y='Predicted_SPEI', hue='Model', data=quarterly_data)\n",
    "#         plt.title('Quarterly Average SPEI Comparison')\n",
    "#         plt.xlabel('Quarter')\n",
    "#         plt.ylabel('Average SPEI')\n",
    "#         plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#     # กราฟ scatter plot กับ R²\n",
    "#     plt.subplot(3, 2, 4)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         sns.scatterplot(x='Actual_SPEI', y='Predicted_SPEI', data=model_data, label=model)\n",
    "#     plt.plot([station_data['Actual_SPEI'].min(), station_data['Actual_SPEI'].max()],\n",
    "#              [station_data['Actual_SPEI'].min(), station_data['Actual_SPEI'].max()],\n",
    "#              color='red', linestyle='--')\n",
    "#     plt.title('Actual vs Predicted SPEI Scatter Plot')\n",
    "#     plt.xlabel('Actual SPEI')\n",
    "#     plt.ylabel('Predicted SPEI')\n",
    "#     plt.legend(loc='upper left')\n",
    "\n",
    "#     # กราฟ Residuals\n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         residuals = model_data['Actual_SPEI'] - model_data['Predicted_SPEI']\n",
    "#         sns.histplot(residuals, kde=True, label=model, bins=30, element='step')\n",
    "#     plt.title('Residuals Distribution')\n",
    "#     plt.xlabel('Residuals')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.legend(loc='upper right')\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "\n",
    "# # สร้างกราฟสำหรับแต่ละสถานีและแต่ละความถี่\n",
    "# for station in combined_df['STATION'].unique():\n",
    "#     station_data = combined_df[combined_df['STATION'] == station]\n",
    "#     for frequency in station_data['Frequency'].unique():\n",
    "#         freq_data = station_data[station_data['Frequency'] == frequency]\n",
    "#         plot_station_comparison(freq_data, station, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEQiDO3gau3l"
   },
   "source": [
    "##Comparison SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Uzm4yo6VD3ic",
    "outputId": "1b94566c-93a5-49fe-bf28-206e37df17d5"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from glob import glob\n",
    "\n",
    "# # อ่านข้อมูลจากไฟล์ CSV ทั้งหมด\n",
    "# file_paths = [\n",
    "#     '/content/predicted_spi_results_ARIMA.csv',\n",
    "#     '/content/predicted_spi_results_DecisionTree.csv',\n",
    "#     '/content/predicted_spi_results_RandomForest.csv',\n",
    "#     '/content/predicted_spi_results_Xgboost.csv',\n",
    "#     '/content/predicted_spi_results_LSTM.csv'\n",
    "# ]\n",
    "\n",
    "# dfs = []\n",
    "# for file_path in file_paths:\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     model_name = file_path.split('_')[-1].split('.')[0]\n",
    "#     df['Model'] = model_name\n",
    "#     dfs.append(df)\n",
    "\n",
    "# # รวมข้อมูลทั้งหมด\n",
    "# combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # แปลง DATETIME เป็น datetime object\n",
    "# combined_df['DATETIME'] = pd.to_datetime(combined_df['DATETIME'])\n",
    "\n",
    "# # ฟังก์ชันสำหรับสร้างกราฟแต่ละสถานี\n",
    "# def plot_station_comparison(station_data, station_name):\n",
    "#     plt.figure(figsize=(18, 12))\n",
    "#     plt.suptitle(f'Station: {station_name} - Comparison of Models', fontsize=16)\n",
    "\n",
    "#     # กราฟเส้นเปรียบเทียบค่าจริงกับค่าทำนาย\n",
    "#     plt.subplot(2, 2, 1)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         plt.plot(model_data['DATETIME'], model_data['Predicted_SPI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "#     actual_data = station_data.drop_duplicates(subset=['DATETIME'])\n",
    "#     plt.plot(actual_data['DATETIME'], actual_data['Actual_SPI'], label='Actual', linewidth=2)\n",
    "\n",
    "#     plt.title('Actual vs Predicted SPI for 2023')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPI')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบ MSE\n",
    "#     plt.subplot(2, 2, 2)\n",
    "#     mse_data = []\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         mse = ((model_data['Actual_SPI'] - model_data['Predicted_SPI'])**2).mean()\n",
    "#         mse_data.append({'Model': model, 'MSE': mse})\n",
    "\n",
    "#     mse_df = pd.DataFrame(mse_data)\n",
    "#     sns.barplot(x='Model', y='MSE', data=mse_df)\n",
    "#     plt.title('Mean Squared Error Comparison')\n",
    "#     plt.xlabel('Model')\n",
    "#     plt.ylabel('MSE')\n",
    "#     plt.xticks(rotation=45)\n",
    "\n",
    "#     # กราฟแท่งเปรียบเทียบค่าเฉลี่ย SPI รายไตรมาส\n",
    "#     plt.subplot(2, 2, 3)\n",
    "#     station_data['Quarter'] = station_data['DATETIME'].dt.to_period('Q')\n",
    "#     quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPI'].mean().reset_index()\n",
    "#     actual_quarterly = station_data.groupby('Quarter')['Actual_SPI'].mean().reset_index()\n",
    "#     actual_quarterly['Model'] = 'Actual'\n",
    "#     quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "#     sns.barplot(x='Quarter', y='Predicted_SPI', hue='Model', data=quarterly_data)\n",
    "#     plt.title('Quarterly Average SPI Comparison')\n",
    "#     plt.xlabel('Quarter')\n",
    "#     plt.ylabel('Average SPI')\n",
    "#     plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # สร้างกราฟสำหรับแต่ละสถานี\n",
    "# for station in combined_df['STATION'].unique():\n",
    "#     station_data = combined_df[combined_df['STATION'] == station]\n",
    "#     plot_station_comparison(station_data, station)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vA99g227YcC7",
    "outputId": "559ea58f-e6dc-4ce4-995b-e413032471fa"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Function to load data and prepare DataFrame\n",
    "# def load_data(file_paths, frequency):\n",
    "#     dfs = []\n",
    "#     for file_path in file_paths:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # Extract model name from the filename\n",
    "#         model_name = file_path.split('_')[-1].split('.')[0]\n",
    "#         df['Model'] = model_name\n",
    "#         df['Frequency'] = frequency\n",
    "#         # Rename columns for consistency\n",
    "#         if frequency == 'Quarterly':\n",
    "#             df.rename(columns={\n",
    "#                 'Quarter': 'Time',\n",
    "#                 'Actual_SPI_Quarterly': 'Actual_SPI',\n",
    "#                 'Predicted_SPI_Quarterly': 'Predicted_SPI'\n",
    "#             }, inplace=True)\n",
    "#             # Convert 'Time' to datetime using pandas.Period\n",
    "#             df['Time'] = df['Time'].apply(lambda x: pd.Period(x, freq='Q').start_time)\n",
    "#         else:\n",
    "#             df.rename(columns={'DATETIME': 'Time'}, inplace=True)\n",
    "#             df['Time'] = pd.to_datetime(df['Time'])\n",
    "#         dfs.append(df)\n",
    "#     combined = pd.concat(dfs, ignore_index=True)\n",
    "#     return combined\n",
    "\n",
    "# # File paths for monthly data\n",
    "# monthly_file_paths = [\n",
    "#     '/content/predicted_spi_results_ARIMA.csv',\n",
    "#     '/content/predicted_spi_results_DecisionTree.csv',\n",
    "#     '/content/predicted_spi_results_RandomForest.csv',\n",
    "#     '/content/predicted_spi_results_Xgboost.csv',\n",
    "#     '/content/predicted_spi_results_LSTM.csv'\n",
    "# ]\n",
    "\n",
    "# # File paths for quarterly data\n",
    "# quarterly_file_paths = [\n",
    "#     '/content/predicted_Quarterly_spi_LSTM.csv',\n",
    "#     '/content/predicted_Quarterly_spi_ARIMA.csv',\n",
    "#     '/content/predicted_Quarterly_spi_RandomForest.csv',\n",
    "#     '/content/predicted_Quarterly_spi_xgboost.csv',\n",
    "#     '/content/predicted_Quarterly_spi_Decisiontree.csv'\n",
    "# ]\n",
    "\n",
    "# # Load monthly and quarterly data\n",
    "# monthly_df = load_data(monthly_file_paths, 'Monthly')\n",
    "# quarterly_df = load_data(quarterly_file_paths, 'Quarterly')\n",
    "\n",
    "# # Combine both types of data\n",
    "# combined_df = pd.concat([monthly_df, quarterly_df], ignore_index=True)\n",
    "\n",
    "# # Function to create plots for each station and frequency\n",
    "# def plot_station_comparison(station_data, station_name, frequency):\n",
    "#     plt.figure(figsize=(18, 18))\n",
    "#     plt.suptitle(f'Station: {station_name} - Comparison of Models (SPI)', fontsize=16)\n",
    "\n",
    "#     # Line plot for actual vs predicted\n",
    "#     plt.subplot(3, 2, 1)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         plt.plot(model_data['Time'], model_data['Predicted_SPI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "#     actual_data = station_data.drop_duplicates(subset=['Time'])\n",
    "#     plt.plot(actual_data['Time'], actual_data['Actual_SPI'], label='Actual', linewidth=2)\n",
    "\n",
    "#     plt.title(f'Actual vs Predicted SPI for {frequency}')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('SPI')\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "#     # Bar plot for MSE, MAE, RMSE, R²\n",
    "#     plt.subplot(3, 2, 2)\n",
    "#     error_data = []\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         mse = mean_squared_error(model_data['Actual_SPI'], model_data['Predicted_SPI'])\n",
    "#         mae = mean_absolute_error(model_data['Actual_SPI'], model_data['Predicted_SPI'])\n",
    "#         rmse = mean_squared_error(model_data['Actual_SPI'], model_data['Predicted_SPI'], squared=False)\n",
    "#         r2 = r2_score(model_data['Actual_SPI'], model_data['Predicted_SPI'])\n",
    "#         error_data.append({'Model': model, 'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "#     error_df = pd.DataFrame(error_data)\n",
    "#     error_melted = error_df.melt(id_vars='Model', value_vars=['MSE', 'MAE', 'RMSE', 'R2'], var_name='Metric', value_name='Value')\n",
    "\n",
    "#     sns.barplot(x='Model', y='Value', hue='Metric', data=error_melted)\n",
    "#     plt.title('Error Metrics Comparison')\n",
    "#     plt.xlabel('Model')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.legend(title='Metric', loc='upper right')\n",
    "\n",
    "#     # Bar plot for quarterly average SPI comparison (only for monthly data)\n",
    "#     if frequency == 'Monthly':\n",
    "#         plt.subplot(3, 2, 3)\n",
    "#         station_data['Quarter'] = station_data['Time'].dt.to_period('Q')\n",
    "#         quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPI'].mean().reset_index()\n",
    "#         actual_quarterly = station_data.groupby('Quarter')['Actual_SPI'].mean().reset_index()\n",
    "#         actual_quarterly['Model'] = 'Actual'\n",
    "#         quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "#         sns.barplot(x='Quarter', y='Predicted_SPI', hue='Model', data=quarterly_data)\n",
    "#         plt.title('Quarterly Average SPI Comparison')\n",
    "#         plt.xlabel('Quarter')\n",
    "#         plt.ylabel('Average SPI')\n",
    "#         plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "#     # Scatter plot with R²\n",
    "#     plt.subplot(3, 2, 4)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         sns.scatterplot(x='Actual_SPI', y='Predicted_SPI', data=model_data, label=model)\n",
    "#     plt.plot([station_data['Actual_SPI'].min(), station_data['Actual_SPI'].max()],\n",
    "#              [station_data['Actual_SPI'].min(), station_data['Actual_SPI'].max()],\n",
    "#              color='red', linestyle='--')\n",
    "#     plt.title('Actual vs Predicted SPI Scatter Plot')\n",
    "#     plt.xlabel('Actual SPI')\n",
    "#     plt.ylabel('Predicted SPI')\n",
    "#     plt.legend(loc='upper left')\n",
    "\n",
    "#     # Residuals plot\n",
    "#     plt.subplot(3, 2, 5)\n",
    "#     for model in station_data['Model'].unique():\n",
    "#         model_data = station_data[station_data['Model'] == model]\n",
    "#         residuals = model_data['Actual_SPI'] - model_data['Predicted_SPI']\n",
    "#         sns.histplot(residuals, kde=True, label=model, bins=30, element='step')\n",
    "#     plt.title('Residuals Distribution')\n",
    "#     plt.xlabel('Residuals')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.legend(loc='upper right')\n",
    "\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.show()\n",
    "\n",
    "# # Create plots for each station and frequency\n",
    "# for station in combined_df['STATION'].unique():\n",
    "#     station_data = combined_df[combined_df['STATION'] == station]\n",
    "#     for frequency in station_data['Frequency'].unique():\n",
    "#         freq_data = station_data[station_data['Frequency'] == frequency]\n",
    "#         plot_station_comparison(freq_data, station, frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2beikwavlM9f"
   },
   "source": [
    "# Comparosion all ความแม่นยำของโมเดล"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIXtTTRYzo0R"
   },
   "source": [
    "## SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EPG0kOXZlWze",
    "outputId": "c9106d54-a47a-469f-a662-830085ea6b4c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# ฟังก์ชันสำหรับอ่านข้อมูลและเตรียม DataFrame\n",
    "def load_data(file_paths, frequency):\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = file_path.split('_')[-1].split('.')[0]\n",
    "        df['Model'] = model_name\n",
    "        df['Frequency'] = frequency\n",
    "        if frequency == 'Quarterly':\n",
    "            df.rename(columns={\n",
    "                'Quarter': 'Time',\n",
    "                'Actual_SPEI_Quarterly': 'Actual_SPEI',\n",
    "                'Predicted_SPEI_Quarterly': 'Predicted_SPEI'\n",
    "            }, inplace=True)\n",
    "            df['Time'] = df['Time'].apply(lambda x: pd.Period(x, freq='Q').start_time)\n",
    "        else:\n",
    "            df.rename(columns={'DATETIME': 'Time'}, inplace=True)\n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "# เส้นทางไฟล์สำหรับข้อมูลรายเดือนและไตรมาส\n",
    "monthly_file_paths = [\n",
    "    '/content/predicted_spei_results_ARIMA.csv',\n",
    "    '/content/predicted_spei_results_Decisiontree.csv',\n",
    "    '/content/predicted_spei_results_RandomForest.csv',\n",
    "    '/content/predicted_spei_results_Xgboost.csv',\n",
    "    '/content/predicted_spei_results_LSTM.csv'\n",
    "]\n",
    "\n",
    "quarterly_file_paths = [\n",
    "    '/content/predicted_Quarterly_spei_LSTM.csv',\n",
    "    '/content/predicted_Quarterly_spei_ARIMA.csv',\n",
    "    '/content/predicted_Quarterly_spei_RandomForest.csv',\n",
    "    '/content/predicted_Quarterly_spei_xgboost.csv',\n",
    "    '/content/predicted_Quarterly_spei_Decisiontree.csv'\n",
    "]\n",
    "\n",
    "monthly_df = load_data(monthly_file_paths, 'Monthly')\n",
    "quarterly_df = load_data(quarterly_file_paths, 'Quarterly')\n",
    "combined_df = pd.concat([monthly_df, quarterly_df], ignore_index=True)\n",
    "\n",
    "# ฟังก์ชันสำหรับคำนวณ metrics และความแม่นยำ\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    # คำนวณความแม่นยำเป็นเปอร์เซ็นต์\n",
    "    accuracy = (1 - mae / (actual.max() - actual.min())) * 100\n",
    "\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างกราฟแต่ละสถานีและแต่ละความถี่\n",
    "def plot_station_comparison(station_data, station_name, frequency):\n",
    "    plt.figure(figsize=(20, 24))\n",
    "    plt.suptitle(f'Station: {station_name} - Comparison of Models ({frequency})', fontsize=16)\n",
    "\n",
    "    # กราฟเส้นเปรียบเทียบค่าจริงกับค่าทำนาย\n",
    "    plt.subplot(4, 2, 1)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        plt.plot(model_data['Time'], model_data['Predicted_SPEI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "    actual_data = station_data.drop_duplicates(subset=['Time'])\n",
    "    plt.plot(actual_data['Time'], actual_data['Actual_SPEI'], label='Actual', linewidth=2)\n",
    "\n",
    "    plt.title(f'Actual vs Predicted SPEI for {frequency}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPEI')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # กราฟแท่งเปรียบเทียบ MSE, MAE, RMSE, R²\n",
    "    plt.subplot(4, 2, 2)\n",
    "    error_data = []\n",
    "    summary_data = []\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        metrics = calculate_metrics(model_data['Actual_SPEI'], model_data['Predicted_SPEI'])\n",
    "        error_data.append({**{'Model': model}, **metrics})\n",
    "        summary_data.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'R2': metrics['R2']\n",
    "        })\n",
    "\n",
    "    error_df = pd.DataFrame(error_data)\n",
    "    error_melted = error_df.melt(id_vars='Model', value_vars=['MSE', 'MAE', 'RMSE', 'R2'], var_name='Metric', value_name='Value')\n",
    "\n",
    "    sns.barplot(x='Model', y='Value', hue='Metric', data=error_melted)\n",
    "    plt.title('Error Metrics Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric', loc='upper right')\n",
    "\n",
    "    # กราฟแท่งเปรียบเทียบค่าเฉลี่ย SPEI รายไตรมาส (เฉพาะข้อมูลแบบรายเดือน)\n",
    "    if frequency == 'Monthly':\n",
    "        plt.subplot(4, 2, 3)\n",
    "        station_data['Quarter'] = pd.PeriodIndex(station_data['Time'], freq='Q')\n",
    "        quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPEI'].mean().reset_index()\n",
    "        actual_quarterly = station_data.groupby('Quarter')['Actual_SPEI'].mean().reset_index()\n",
    "        actual_quarterly['Model'] = 'Actual'\n",
    "        quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "        sns.barplot(x='Quarter', y='Predicted_SPEI', hue='Model', data=quarterly_data)\n",
    "        plt.title('Quarterly Average SPEI Comparison')\n",
    "        plt.xlabel('Quarter')\n",
    "        plt.ylabel('Average SPEI')\n",
    "        plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # กราฟ scatter plot กับ R²\n",
    "    plt.subplot(4, 2, 4)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        sns.scatterplot(x='Actual_SPEI', y='Predicted_SPEI', data=model_data, label=model)\n",
    "    plt.plot([station_data['Actual_SPEI'].min(), station_data['Actual_SPEI'].max()],\n",
    "             [station_data['Actual_SPEI'].min(), station_data['Actual_SPEI'].max()],\n",
    "             color='red', linestyle='--')\n",
    "    plt.title('Actual vs Predicted SPEI Scatter Plot')\n",
    "    plt.xlabel('Actual SPEI')\n",
    "    plt.ylabel('Predicted SPEI')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # กราฟ Residuals\n",
    "    plt.subplot(4, 2, 5)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        residuals = model_data['Actual_SPEI'] - model_data['Predicted_SPEI']\n",
    "        sns.histplot(residuals, kde=True, label=model, bins=30, element='step')\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # เพิ่มตารางสรุปผลความแม่นยำ\n",
    "    plt.subplot(4, 2, 6)\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    plt.axis('off')\n",
    "    cell_text = [[f\"{row['Model']}\", f\"{row['Accuracy']:.2f}%\", f\"{row['R2']:.4f}\"] for _, row in summary_df.iterrows()]\n",
    "    table = plt.table(cellText=cell_text, colLabels=['Model', 'Accuracy', 'R2'], loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    plt.title('Model Accuracy Summary', fontsize=12)\n",
    "\n",
    "    # เพิ่มกราฟแท่งแสดงความแม่นยำ\n",
    "    plt.subplot(4, 2, 7)\n",
    "    sns.barplot(x='Model', y='Accuracy', data=summary_df)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # พิมพ์สรุปผลความแม่นยำ\n",
    "    print(f\"\\nSummary for {station_name} - {frequency}:\")\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"{row['Model']}: Accuracy = {row['Accuracy']:.2f}%, R² = {row['R2']:.4f}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# สร้างกราฟและเก็บผลสรุปสำหรับแต่ละสถานีและแต่ละความถี่\n",
    "all_summaries = []\n",
    "for station in combined_df['STATION'].unique():\n",
    "    station_data = combined_df[combined_df['STATION'] == station]\n",
    "    for frequency in station_data['Frequency'].unique():\n",
    "        freq_data = station_data[station_data['Frequency'] == frequency]\n",
    "        summary = plot_station_comparison(freq_data, station, frequency)\n",
    "        summary['Station'] = station\n",
    "        summary['Frequency'] = frequency\n",
    "        all_summaries.append(summary)\n",
    "\n",
    "# รวมผลสรุปทั้งหมด\n",
    "final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "# แสดงผลสรุปรวมทั้งหมด\n",
    "print(\"\\nOverall Summary:\")\n",
    "print(final_summary.groupby(['Frequency', 'Model'])[['Accuracy', 'R2']].mean())\n",
    "\n",
    "# สร้างกราฟสรุปผลรวมทั้งหมด\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Frequency', data=final_summary)\n",
    "plt.title('Overall Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkNHSR8EzvOZ"
   },
   "source": [
    "## SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YA5MkvzorJGz",
    "outputId": "feae2c52-0616-407a-b243-d97e409d0b42"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# ฟังก์ชันสำหรับอ่านข้อมูลและเตรียม DataFrame\n",
    "def load_data(file_paths, frequency):\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        model_name = file_path.split('_')[-1].split('.')[0]\n",
    "        df['Model'] = model_name\n",
    "        df['Frequency'] = frequency\n",
    "        if frequency == 'Quarterly':\n",
    "            df.rename(columns={\n",
    "                'Quarter': 'Time',\n",
    "                'Actual_SPI_Quarterly': 'Actual_SPI',\n",
    "                'Predicted_SPI_Quarterly': 'Predicted_SPI'\n",
    "            }, inplace=True)\n",
    "            df['Time'] = df['Time'].apply(lambda x: pd.Period(x, freq='Q').start_time)\n",
    "        else:\n",
    "            df.rename(columns={'DATETIME': 'Time'}, inplace=True)\n",
    "            df['Time'] = pd.to_datetime(df['Time'])\n",
    "        dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "# เส้นทางไฟล์สำหรับข้อมูลรายเดือนและไตรมาส\n",
    "monthly_file_paths = [\n",
    "    '/content/predicted_spi_results_ARIMA.csv',\n",
    "    '/content/predicted_spi_results_DecisionTree.csv',\n",
    "    '/content/predicted_spi_results_RandomForest.csv',\n",
    "    '/content/predicted_spi_results_Xgboost.csv',\n",
    "    '/content/predicted_spi_results_LSTM.csv'\n",
    "]\n",
    "\n",
    "quarterly_file_paths = [\n",
    "    '/content/predicted_Quarterly_spi_LSTM.csv',\n",
    "    '/content/predicted_Quarterly_spi_ARIMA.csv',\n",
    "    '/content/predicted_Quarterly_spi_RandomForest.csv',\n",
    "    '/content/predicted_Quarterly_spi_xgboost.csv',\n",
    "    '/content/predicted_Quarterly_spi_Decisiontree.csv'\n",
    "]\n",
    "\n",
    "monthly_df = load_data(monthly_file_paths, 'Monthly')\n",
    "quarterly_df = load_data(quarterly_file_paths, 'Quarterly')\n",
    "combined_df = pd.concat([monthly_df, quarterly_df], ignore_index=True)\n",
    "\n",
    "# ฟังก์ชันสำหรับคำนวณ metrics และความแม่นยำ\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "\n",
    "    # คำนวณความแม่นยำเป็นเปอร์เซ็นต์\n",
    "    accuracy = (1 - mae / (actual.max() - actual.min())) * 100\n",
    "\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# ฟังก์ชันสำหรับสร้างกราฟแต่ละสถานีและแต่ละความถี่\n",
    "def plot_station_comparison(station_data, station_name, frequency):\n",
    "    plt.figure(figsize=(20, 24))\n",
    "    plt.suptitle(f'Station: {station_name} - Comparison of Models ({frequency})', fontsize=16)\n",
    "\n",
    "    # กราฟเส้นเปรียบเทียบค่าจริงกับค่าทำนาย\n",
    "    plt.subplot(4, 2, 1)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        plt.plot(model_data['Time'], model_data['Predicted_SPI'], label=f'Predicted ({model})', linestyle='--')\n",
    "\n",
    "    actual_data = station_data.drop_duplicates(subset=['Time'])\n",
    "    plt.plot(actual_data['Time'], actual_data['Actual_SPI'], label='Actual', linewidth=2)\n",
    "\n",
    "    plt.title(f'Actual vs Predicted SPI for {frequency}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SPI')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # กราฟแท่งเปรียบเทียบ MSE, MAE, RMSE, R²\n",
    "    plt.subplot(4, 2, 2)\n",
    "    error_data = []\n",
    "    summary_data = []\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        metrics = calculate_metrics(model_data['Actual_SPI'], model_data['Predicted_SPI'])\n",
    "        error_data.append({**{'Model': model}, **metrics})\n",
    "        summary_data.append({\n",
    "            'Model': model,\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'R2': metrics['R2']\n",
    "        })\n",
    "\n",
    "    error_df = pd.DataFrame(error_data)\n",
    "    error_melted = error_df.melt(id_vars='Model', value_vars=['MSE', 'MAE', 'RMSE', 'R2'], var_name='Metric', value_name='Value')\n",
    "\n",
    "    sns.barplot(x='Model', y='Value', hue='Metric', data=error_melted)\n",
    "    plt.title('Error Metrics Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric', loc='upper right')\n",
    "\n",
    "    # กราฟแท่งเปรียบเทียบค่าเฉลี่ย SPEI รายไตรมาส (เฉพาะข้อมูลแบบรายเดือน)\n",
    "    if frequency == 'Monthly':\n",
    "        plt.subplot(4, 2, 3)\n",
    "        station_data['Quarter'] = pd.PeriodIndex(station_data['Time'], freq='Q')\n",
    "        quarterly_data = station_data.groupby(['Quarter', 'Model'])['Predicted_SPI'].mean().reset_index()\n",
    "        actual_quarterly = station_data.groupby('Quarter')['Actual_SPI'].mean().reset_index()\n",
    "        actual_quarterly['Model'] = 'Actual'\n",
    "        quarterly_data = pd.concat([quarterly_data, actual_quarterly], ignore_index=True)\n",
    "\n",
    "        sns.barplot(x='Quarter', y='Predicted_SPI', hue='Model', data=quarterly_data)\n",
    "        plt.title('Quarterly Average SPI Comparison')\n",
    "        plt.xlabel('Quarter')\n",
    "        plt.ylabel('Average SPI')\n",
    "        plt.legend(title='Model', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # กราฟ scatter plot กับ R²\n",
    "    plt.subplot(4, 2, 4)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        sns.scatterplot(x='Actual_SPI', y='Predicted_SPI', data=model_data, label=model)\n",
    "    plt.plot([station_data['Actual_SPI'].min(), station_data['Actual_SPI'].max()],\n",
    "             [station_data['Actual_SPI'].min(), station_data['Actual_SPI'].max()],\n",
    "             color='red', linestyle='--')\n",
    "    plt.title('Actual vs Predicted SPI Scatter Plot')\n",
    "    plt.xlabel('Actual SPI')\n",
    "    plt.ylabel('Predicted SPI')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    # กราฟ Residuals\n",
    "    plt.subplot(4, 2, 5)\n",
    "    for model in station_data['Model'].unique():\n",
    "        model_data = station_data[station_data['Model'] == model]\n",
    "        residuals = model_data['Actual_SPI'] - model_data['Predicted_SPI']\n",
    "        sns.histplot(residuals, kde=True, label=model, bins=30, element='step')\n",
    "    plt.title('Residuals Distribution')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # เพิ่มตารางสรุปผลความแม่นยำ\n",
    "    plt.subplot(4, 2, 6)\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    plt.axis('off')\n",
    "    cell_text = [[f\"{row['Model']}\", f\"{row['Accuracy']:.2f}%\", f\"{row['R2']:.4f}\"] for _, row in summary_df.iterrows()]\n",
    "    table = plt.table(cellText=cell_text, colLabels=['Model', 'Accuracy', 'R2'], loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    plt.title('Model Accuracy Summary', fontsize=12)\n",
    "\n",
    "    # เพิ่มกราฟแท่งแสดงความแม่นยำ\n",
    "    plt.subplot(4, 2, 7)\n",
    "    sns.barplot(x='Model', y='Accuracy', data=summary_df)\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # พิมพ์สรุปผลความแม่นยำ\n",
    "    print(f\"\\nSummary for {station_name} - {frequency}:\")\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"{row['Model']}: Accuracy = {row['Accuracy']:.2f}%, R² = {row['R2']:.4f}\")\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# สร้างกราฟและเก็บผลสรุปสำหรับแต่ละสถานีและแต่ละความถี่\n",
    "all_summaries = []\n",
    "for station in combined_df['STATION'].unique():\n",
    "    station_data = combined_df[combined_df['STATION'] == station]\n",
    "    for frequency in station_data['Frequency'].unique():\n",
    "        freq_data = station_data[station_data['Frequency'] == frequency]\n",
    "        summary = plot_station_comparison(freq_data, station, frequency)\n",
    "        summary['Station'] = station\n",
    "        summary['Frequency'] = frequency\n",
    "        all_summaries.append(summary)\n",
    "\n",
    "# รวมผลสรุปทั้งหมด\n",
    "final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "# แสดงผลสรุปรวมทั้งหมด\n",
    "print(\"\\nOverall Summary:\")\n",
    "print(final_summary.groupby(['Frequency', 'Model'])[['Accuracy', 'R2']].mean())\n",
    "\n",
    "# สร้างกราฟสรุปผลรวมทั้งหมด\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Frequency', data=final_summary)\n",
    "plt.title('Overall Model Accuracy Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tm5sYZnt-dnV",
    "I7D3BvLfadph",
    "COaXIv6FS5rQ",
    "tK_lDvvFS6SY",
    "Z_5LEnPAsO_k",
    "WZqBIWp1RLWc",
    "h7pL4ndUDhNW",
    "xwwsfMWyRUQw",
    "pV1RUHvKIxsO",
    "qNqYYRt9QHMU",
    "74Dqhrq5TSyh",
    "-l3s9zeaVEXS"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
